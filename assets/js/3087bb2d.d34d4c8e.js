"use strict";(self.webpackChunkfastkafka=self.webpackChunkfastkafka||[]).push([[6308],{3905:(e,n,t)=>{t.d(n,{Zo:()=>l,kt:()=>f});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function r(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},s=Object.keys(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(a=0;a<s.length;a++)t=s[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var d=a.createContext({}),c=function(e){var n=a.useContext(d),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},l=function(e){var n=c(e.components);return a.createElement(d.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},u=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,s=e.originalType,d=e.parentName,l=r(e,["components","mdxType","originalType","parentName"]),p=c(t),u=i,f=p["".concat(d,".").concat(u)]||p[u]||m[u]||s;return t?a.createElement(f,o(o({ref:n},l),{},{components:t})):a.createElement(f,o({ref:n},l))}));function f(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var s=t.length,o=new Array(s);o[0]=u;var r={};for(var d in n)hasOwnProperty.call(n,d)&&(r[d]=n[d]);r.originalType=e,r[p]="string"==typeof e?e:i,o[1]=r;for(var c=2;c<s;c++)o[c]=t[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}u.displayName="MDXCreateElement"},7247:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var a=t(7462),i=(t(7294),t(3905));const s={},o="Encoding and Decoding Kafka Messages with FastKafka",r={unversionedId:"guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka",id:"version-0.7.0/guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka",title:"Encoding and Decoding Kafka Messages with FastKafka",description:"Prerequisites",source:"@site/versioned_docs/version-0.7.0/guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka.md",sourceDirName:"guides",slug:"/guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka",permalink:"/docs/0.7.0/guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka",draft:!1,tags:[],version:"0.7.0",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Lifespan Events",permalink:"/docs/0.7.0/guides/Guide_05_Lifespan_Handler"},next:{title:"Using multiple Kafka clusters",permalink:"/docs/0.7.0/guides/Guide_24_Using_Multiple_Kafka_Clusters"}},d={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Ways to Encode and Decode Messages with FastKafka",id:"ways-to-encode-and-decode-messages-with-fastkafka",level:2},{value:"1. Json encoder and decoder",id:"1-json-encoder-and-decoder",level:2},{value:"2. Avro encoder and decoder",id:"2-avro-encoder-and-decoder",level:2},{value:"What is Avro?",id:"what-is-avro",level:3},{value:"Installing FastKafka with Avro dependencies",id:"installing-fastkafka-with-avro-dependencies",level:3},{value:"Defining Avro Schema Using Pydantic Models",id:"defining-avro-schema-using-pydantic-models",level:3},{value:"Reusing existing avro schema",id:"reusing-existing-avro-schema",level:3},{value:"Building pydantic models from avro schema dictionary",id:"building-pydantic-models-from-avro-schema-dictionary",level:4},{value:"Building pydantic models from <code>.avsc</code> file",id:"building-pydantic-models-from-avsc-file",level:4},{value:"Consume/Produce avro messages with FastKafka",id:"consumeproduce-avro-messages-with-fastkafka",level:3},{value:"Assembling it all together",id:"assembling-it-all-together",level:3},{value:"3. Custom encoder and decoder",id:"3-custom-encoder-and-decoder",level:2},{value:"Writing a custom encoder and decoder",id:"writing-a-custom-encoder-and-decoder",level:3},{value:"Assembling it all together",id:"assembling-it-all-together-1",level:3}],l={toc:c},p="wrapper";function m(e){let{components:n,...t}=e;return(0,i.kt)(p,(0,a.Z)({},l,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"encoding-and-decoding-kafka-messages-with-fastkafka"},"Encoding and Decoding Kafka Messages with FastKafka"),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"A basic knowledge of ",(0,i.kt)("inlineCode",{parentName:"li"},"FastKafka")," is needed to proceed with this\nguide. If you are not familiar with ",(0,i.kt)("inlineCode",{parentName:"li"},"FastKafka"),", please go through\nthe ",(0,i.kt)("a",{parentName:"li",href:"/docs#tutorial"},"tutorial")," first."),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("inlineCode",{parentName:"li"},"FastKafka")," with its dependencies installed is needed. Please\ninstall ",(0,i.kt)("inlineCode",{parentName:"li"},"FastKafka")," using the command - ",(0,i.kt)("inlineCode",{parentName:"li"},"pip install fastkafka"))),(0,i.kt)("h2",{id:"ways-to-encode-and-decode-messages-with-fastkafka"},"Ways to Encode and Decode Messages with FastKafka"),(0,i.kt)("p",null,"In python, by default, we send Kafka messages as bytes. Even if our\nmessage is a string, we convert it to bytes and then send it to Kafka\ntopic. imilarly, while consuming messages, we consume them as bytes and\nthen convert them to strings."),(0,i.kt)("p",null,"In FastKafka, we specify message schema using Pydantic models as\nmentioned in ",(0,i.kt)("a",{parentName:"p",href:"/docs#messages"},"tutorial"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Define Pydantic models for Kafka messages\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n')),(0,i.kt)("p",null,"Then, we send and receive messages as instances of Pydantic models which\nwe defined. So, FastKafka needs a way to encode/decode to these Pydantic\nmodel messages to bytes in order to send/receive messages to/from Kafka\ntopics."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"@consumes")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"@produces")," methods of FastKafka accept a parameter\ncalled ",(0,i.kt)("inlineCode",{parentName:"p"},"decoder"),"/",(0,i.kt)("inlineCode",{parentName:"p"},"encoder")," to decode/encode Kafka messages. FastKafka\nprovides three ways to encode and decode messages:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"json - This is the default encoder/decoder option in FastKafka.\nWhile producing, this option converts our instance of Pydantic model\nmessages to a JSON string and then converts it to bytes before\nsending it to the topic. While consuming, it converts bytes to a\nJSON string and then constructs an instance of Pydantic model from\nthe JSON string."),(0,i.kt)("li",{parentName:"ol"},"avro - This option uses Avro encoding/decoding to convert instances\nof Pydantic model messages to bytes while producing, and while\nconsuming, it constructs an instance of Pydantic model from bytes."),(0,i.kt)("li",{parentName:"ol"},"custom encoder/decoder - If you are not happy with the json or avro\nencoder/decoder options, you can write your own encoder/decoder\nfunctions and use them to encode/decode Pydantic messages.")),(0,i.kt)("h2",{id:"1-json-encoder-and-decoder"},"1. Json encoder and decoder"),(0,i.kt)("p",null,"The default option in FastKafka is json encoder/decoder. This option,\nwhile producing, converts our instance of pydantic model messages to\njson string and then converts to bytes before sending it to the topics.\nWhile consuming it converts bytes to json string and then constructs\ninstance of pydantic model from json string."),(0,i.kt)("p",null,"We can use the application from ",(0,i.kt)("a",{parentName:"p",href:"/docs#running-the-service"},"tutorial")," as\nis, and it will use the json encoder/decoder by default. But, for\nclarity, let\u2019s modify it to explicitly accept the \u2018json\u2019 encoder/decoder\nparameter:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# content of the "application.py" file\n\nfrom contextlib import asynccontextmanager\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nfrom fastkafka import FastKafka\n\nml_models = {}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastKafka):\n    # Load the ML model\n    X, y = load_iris(return_X_y=True)\n    ml_models["iris_predictor"] = LogisticRegression(random_state=0, max_iter=500).fit(\n        X, y\n    )\n    yield\n    # Clean up the ML models and release the resources\n    ml_models.clear()\n\n\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n    \nfrom fastkafka import FastKafka\n\nkafka_brokers = {\n    "localhost": {\n        "url": "localhost",\n        "description": "local development kafka broker",\n        "port": 9092,\n    },\n    "production": {\n        "url": "kafka.airt.ai",\n        "description": "production kafka broker",\n        "port": 9092,\n        "protocol": "kafka-secure",\n        "security": {"type": "plain"},\n    },\n}\n\nkafka_app = FastKafka(\n    title="Iris predictions",\n    kafka_brokers=kafka_brokers,\n    lifespan=lifespan,\n)\n\n@kafka_app.consumes(topic="input_data", decoder="json")\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions", encoder="json")\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,"In the above code, the ",(0,i.kt)("inlineCode",{parentName:"p"},"@kafka_app.consumes"),' decorator sets up a\nconsumer for the \u201cinput_data" topic, using the \u2018json\u2019 decoder to convert\nthe message payload to an instance of ',(0,i.kt)("inlineCode",{parentName:"p"},"IrisInputData"),". The\n",(0,i.kt)("inlineCode",{parentName:"p"},"@kafka_app.produces"),' decorator sets up a producer for the \u201cpredictions"\ntopic, using the \u2018json\u2019 encoder to convert the instance of\n',(0,i.kt)("inlineCode",{parentName:"p"},"IrisPrediction")," to message payload."),(0,i.kt)("h2",{id:"2-avro-encoder-and-decoder"},"2. Avro encoder and decoder"),(0,i.kt)("h3",{id:"what-is-avro"},"What is Avro?"),(0,i.kt)("p",null,"Avro is a row-oriented remote procedure call and data serialization\nframework developed within Apache\u2019s Hadoop project. It uses JSON for\ndefining data types and protocols, and serializes data in a compact\nbinary format. To learn more about the Apache Avro, please check out the\n",(0,i.kt)("a",{parentName:"p",href:"https://avro.apache.org/docs/"},"docs"),"."),(0,i.kt)("h3",{id:"installing-fastkafka-with-avro-dependencies"},"Installing FastKafka with Avro dependencies"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," with dependencies for Apache Avro installed is needed to use\navro encoder/decoder. Please install ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," with Avro support using\nthe command - ",(0,i.kt)("inlineCode",{parentName:"p"},"pip install fastkafka[avro]")),(0,i.kt)("h3",{id:"defining-avro-schema-using-pydantic-models"},"Defining Avro Schema Using Pydantic Models"),(0,i.kt)("p",null,"By default, you can use Pydantic model to define your message schemas.\nFastKafka internally takes care of encoding and decoding avro messages,\nbased on the Pydantic models."),(0,i.kt)("p",null,"So, similar to the ",(0,i.kt)("a",{parentName:"p",href:"/docs#tutorial"},"tutorial"),", the message schema will\nremain as it is."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# Define Pydantic models for Avro messages\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n')),(0,i.kt)("p",null,"No need to change anything to support avro. You can use existing\nPydantic models as is."),(0,i.kt)("h3",{id:"reusing-existing-avro-schema"},"Reusing existing avro schema"),(0,i.kt)("p",null,"If you are using some other library to send and receive avro encoded\nmessages, it is highly likely that you already have an Avro schema\ndefined."),(0,i.kt)("h4",{id:"building-pydantic-models-from-avro-schema-dictionary"},"Building pydantic models from avro schema dictionary"),(0,i.kt)("p",null,"Let\u2019s modify the above example and let\u2019s assume we have schemas already\nfor ",(0,i.kt)("inlineCode",{parentName:"p"},"IrisInputData")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"IrisPrediction")," which will look like below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'iris_input_data_schema = {\n    "type": "record",\n    "namespace": "IrisInputData",\n    "name": "IrisInputData",\n    "fields": [\n        {"doc": "Sepal length in cm", "type": "double", "name": "sepal_length"},\n        {"doc": "Sepal width in cm", "type": "double", "name": "sepal_width"},\n        {"doc": "Petal length in cm", "type": "double", "name": "petal_length"},\n        {"doc": "Petal width in cm", "type": "double", "name": "petal_width"},\n    ],\n}\niris_prediction_schema = {\n    "type": "record",\n    "namespace": "IrisPrediction",\n    "name": "IrisPrediction",\n    "fields": [{"doc": "Predicted species", "type": "string", "name": "species"}],\n}\n')),(0,i.kt)("p",null,"We can easily construct pydantic models from avro schema using\n",(0,i.kt)("inlineCode",{parentName:"p"},"avsc_to_pydantic")," function which is included as part of ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka"),"\nitself."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"from fastkafka.encoder import avsc_to_pydantic\n\nIrisInputData = avsc_to_pydantic(iris_input_data_schema)\nprint(IrisInputData.__fields__)\n\nIrisPrediction = avsc_to_pydantic(iris_prediction_schema)\nprint(IrisPrediction.__fields__)\n")),(0,i.kt)("p",null,"The above code will convert avro schema to pydantic models and will\nprint pydantic models\u2019 fields. The output of the above is:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},"{'sepal_length': ModelField(name='sepal_length', type=float, required=True),\n 'sepal_width': ModelField(name='sepal_width', type=float, required=True),\n 'petal_length': ModelField(name='petal_length', type=float, required=True),\n 'petal_width': ModelField(name='petal_width', type=float, required=True)}\n \n {'species': ModelField(name='species', type=str, required=True)}\n")),(0,i.kt)("p",null,"This is exactly same as manually defining the pydantic models ourselves.\nYou don\u2019t have to worry about not making any mistakes while converting\navro schema to pydantic models manually. You can easily and\nautomatically accomplish it by using ",(0,i.kt)("inlineCode",{parentName:"p"},"avsc_to_pydantic")," function as\ndemonstrated above."),(0,i.kt)("h4",{id:"building-pydantic-models-from-avsc-file"},"Building pydantic models from ",(0,i.kt)("inlineCode",{parentName:"h4"},".avsc")," file"),(0,i.kt)("p",null,"Not all cases will have avro schema conveniently defined as a python\ndictionary. You may have it stored as the proprietary ",(0,i.kt)("inlineCode",{parentName:"p"},".avsc")," files in\nfilesystem. Let\u2019s see how to convert those ",(0,i.kt)("inlineCode",{parentName:"p"},".avsc")," files to pydantic\nmodels."),(0,i.kt)("p",null,"Let\u2019s assume our avro files are stored in files called\n",(0,i.kt)("inlineCode",{parentName:"p"},"iris_input_data_schema.avsc")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"iris_prediction_schema.avsc"),". In that\ncase, following code converts the schema to pydantic models:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import json\nfrom fastkafka.encoder import avsc_to_pydantic\n\n\nwith open("iris_input_data_schema.avsc", "rb") as f:\n    iris_input_data_schema = json.load(f)\n    \nwith open("iris_prediction_schema.avsc", "rb") as f:\n    iris_prediction_schema = json.load(f)\n    \n\nIrisInputData = avsc_to_pydantic(iris_input_data_schema)\nprint(IrisInputData.__fields__)\n\nIrisPrediction = avsc_to_pydantic(iris_prediction_schema)\nprint(IrisPrediction.__fields__)\n')),(0,i.kt)("h3",{id:"consumeproduce-avro-messages-with-fastkafka"},"Consume/Produce avro messages with FastKafka"),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," provides ",(0,i.kt)("inlineCode",{parentName:"p"},"@consumes")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"@produces")," methods to\nconsume/produces messages to/from a ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," topic. This is explained in\n",(0,i.kt)("a",{parentName:"p",href:"/docs#function-decorators"},"tutorial"),"."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"@consumes")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"@produces")," methods accepts a parameter called\n",(0,i.kt)("inlineCode",{parentName:"p"},"decoder"),"/",(0,i.kt)("inlineCode",{parentName:"p"},"encoder")," to decode/encode avro messages."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'@kafka_app.consumes(topic="input_data", encoder="avro")\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions", decoder="avro")\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,"In the above example, in ",(0,i.kt)("inlineCode",{parentName:"p"},"@consumes")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"@produces")," methods, we\nexplicitly instruct FastKafka to ",(0,i.kt)("inlineCode",{parentName:"p"},"decode")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"encode")," messages using\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"avro")," ",(0,i.kt)("inlineCode",{parentName:"p"},"decoder"),"/",(0,i.kt)("inlineCode",{parentName:"p"},"encoder")," instead of the default ",(0,i.kt)("inlineCode",{parentName:"p"},"json"),"\n",(0,i.kt)("inlineCode",{parentName:"p"},"decoder"),"/",(0,i.kt)("inlineCode",{parentName:"p"},"encoder"),"."),(0,i.kt)("h3",{id:"assembling-it-all-together"},"Assembling it all together"),(0,i.kt)("p",null,"Let\u2019s rewrite the sample code found in\n",(0,i.kt)("a",{parentName:"p",href:"/docs#running-the-service"},"tutorial")," to use ",(0,i.kt)("inlineCode",{parentName:"p"},"avro")," to ",(0,i.kt)("inlineCode",{parentName:"p"},"decode")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"encode")," messages:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# content of the "application.py" file\n\nfrom contextlib import asynccontextmanager\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nfrom fastkafka import FastKafka\n\nml_models = {}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastKafka):\n    # Load the ML model\n    X, y = load_iris(return_X_y=True)\n    ml_models["iris_predictor"] = LogisticRegression(random_state=0, max_iter=500).fit(\n        X, y\n    )\n    yield\n    # Clean up the ML models and release the resources\n    ml_models.clear()\n\n\niris_input_data_schema = {\n    "type": "record",\n    "namespace": "IrisInputData",\n    "name": "IrisInputData",\n    "fields": [\n        {"doc": "Sepal length in cm", "type": "double", "name": "sepal_length"},\n        {"doc": "Sepal width in cm", "type": "double", "name": "sepal_width"},\n        {"doc": "Petal length in cm", "type": "double", "name": "petal_length"},\n        {"doc": "Petal width in cm", "type": "double", "name": "petal_width"},\n    ],\n}\niris_prediction_schema = {\n    "type": "record",\n    "namespace": "IrisPrediction",\n    "name": "IrisPrediction",\n    "fields": [{"doc": "Predicted species", "type": "string", "name": "species"}],\n}\n# Or load schema from avsc files\n\nfrom fastkafka.encoder import avsc_to_pydantic\n\nIrisInputData = avsc_to_pydantic(iris_input_data_schema)\nIrisPrediction = avsc_to_pydantic(iris_prediction_schema)\n\n    \nfrom fastkafka import FastKafka\n\nkafka_brokers = {\n    "localhost": {\n        "url": "localhost",\n        "description": "local development kafka broker",\n        "port": 9092,\n    },\n    "production": {\n        "url": "kafka.airt.ai",\n        "description": "production kafka broker",\n        "port": 9092,\n        "protocol": "kafka-secure",\n        "security": {"type": "plain"},\n    },\n}\n\nkafka_app = FastKafka(\n    title="Iris predictions",\n    kafka_brokers=kafka_brokers,\n    lifespan=lifespan,\n)\n\n@kafka_app.consumes(topic="input_data", decoder="avro")\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions", encoder="avro")\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,"The above code is a sample implementation of using FastKafka to consume\nand produce Avro-encoded messages from/to a Kafka topic. The code\ndefines two Avro schemas for the input data and the prediction result.\nIt then uses the ",(0,i.kt)("inlineCode",{parentName:"p"},"avsc_to_pydantic")," function from the FastKafka library\nto convert the Avro schema into Pydantic models, which will be used to\ndecode and encode Avro messages."),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," class is then instantiated with the broker details, and\ntwo functions decorated with ",(0,i.kt)("inlineCode",{parentName:"p"},"@kafka_app.consumes")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"@kafka_app.produces"),' are defined to consume messages from the\n\u201cinput_data" topic and produce messages to the \u201cpredictions" topic,\nrespectively. The functions uses the decoder=\u201cavro" and encoder=\u201cavro"\nparameters to decode and encode the Avro messages.'),(0,i.kt)("p",null,"In summary, the above code demonstrates a straightforward way to use\nAvro-encoded messages with FastKafka to build a message processing\npipeline."),(0,i.kt)("h2",{id:"3-custom-encoder-and-decoder"},"3. Custom encoder and decoder"),(0,i.kt)("p",null,"If you are not happy with the json or avro encoder/decoder options, you\ncan write your own encoder/decoder functions and use them to\nencode/decode Pydantic messages."),(0,i.kt)("h3",{id:"writing-a-custom-encoder-and-decoder"},"Writing a custom encoder and decoder"),(0,i.kt)("p",null,"In this section, let\u2019s see how to write a custom encoder and decoder\nwhich obfuscates kafka message with simple\n",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/ROT13"},"ROT13")," cipher."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"import codecs\nimport json\nfrom typing import Any\n\nfrom pydantic.main import ModelMetaclass\n\n\ndef custom_encoder(msg: BaseModel) -> bytes:\n    msg_str = msg.json()\n    obfuscated = codecs.encode(msg_str, 'rot13')\n    raw_bytes = obfuscated.encode(\"utf-8\")\n    return raw_bytes\n\ndef custom_decoder(raw_msg: bytes, cls: ModelMetaclass) -> Any:\n    obfuscated = raw_msg.decode(\"utf-8\")\n    msg_str = codecs.decode(obfuscated, 'rot13')\n    msg_dict = json.loads(msg_str)\n    return cls(**msg_dict)\n")),(0,i.kt)("p",null,"The above code defines two custom functions for encoding and decoding\nmessages in a Kafka application using the FastKafka library."),(0,i.kt)("p",null,"The encoding function, ",(0,i.kt)("inlineCode",{parentName:"p"},"custom_encoder()"),", takes a message ",(0,i.kt)("inlineCode",{parentName:"p"},"msg")," which\nis an instance of a Pydantic model, converts it to a JSON string using\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"json()")," method, obfuscates the resulting string using the ROT13\nalgorithm from the ",(0,i.kt)("inlineCode",{parentName:"p"},"codecs")," module, and finally encodes the obfuscated\nstring as raw bytes using the UTF-8 encoding."),(0,i.kt)("p",null,"The decoding function, ",(0,i.kt)("inlineCode",{parentName:"p"},"custom_decoder()"),", takes a raw message ",(0,i.kt)("inlineCode",{parentName:"p"},"raw_msg"),"\nin bytes format, a Pydantic class to construct instance with cls\nparameter. It first decodes the raw message from UTF-8 encoding, then\nuses the ROT13 algorithm to de-obfuscate the string. Finally, it loads\nthe resulting JSON string using the ",(0,i.kt)("inlineCode",{parentName:"p"},"json.loads()")," method and returns a\nnew instance of the specified ",(0,i.kt)("inlineCode",{parentName:"p"},"cls")," class initialized with the decoded\ndictionary."),(0,i.kt)("p",null,"These functions can be used with FastKafka\u2019s ",(0,i.kt)("inlineCode",{parentName:"p"},"encoder")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"decoder"),"\nparameters to customize the serialization and deserialization of\nmessages in Kafka topics."),(0,i.kt)("p",null,"Let\u2019s test the above code"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"i = IrisInputData(sepal_length=0.5, sepal_width=0.5, petal_length=0.5, petal_width=0.5)\n\nencoded = custom_encoder(i)\ndisplay(encoded)\n\ndecoded = custom_decoder(encoded, IrisInputData)\ndisplay(decoded)\n")),(0,i.kt)("p",null,"This will result in following output"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},'b\'{"frcny_yratgu": 0.5, "frcny_jvqgu": 0.5, "crgny_yratgu": 0.5, "crgny_jvqgu": 0.5}\'\n\nIrisInputData(sepal_length=0.5, sepal_width=0.5, petal_length=0.5, petal_width=0.5)\n')),(0,i.kt)("h3",{id:"assembling-it-all-together-1"},"Assembling it all together"),(0,i.kt)("p",null,"Let\u2019s rewrite the sample code found in\n",(0,i.kt)("a",{parentName:"p",href:"/docs#running-the-service"},"tutorial")," to use our custom decoder and\nencoder functions:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# content of the "application.py" file\n\nfrom contextlib import asynccontextmanager\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nfrom fastkafka import FastKafka\n\nml_models = {}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastKafka):\n    # Load the ML model\n    X, y = load_iris(return_X_y=True)\n    ml_models["iris_predictor"] = LogisticRegression(random_state=0, max_iter=500).fit(\n        X, y\n    )\n    yield\n    # Clean up the ML models and release the resources\n    ml_models.clear()\n\n\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n\n\nimport codecs\nimport json\nfrom typing import Any\n\nfrom pydantic.main import ModelMetaclass\n\n\ndef custom_encoder(msg: BaseModel) -> bytes:\n    msg_str = msg.json()\n    obfuscated = codecs.encode(msg_str, \'rot13\')\n    raw_bytes = obfuscated.encode("utf-8")\n    return raw_bytes\n\ndef custom_decoder(raw_msg: bytes, cls: ModelMetaclass) -> Any:\n    obfuscated = raw_msg.decode("utf-8")\n    msg_str = codecs.decode(obfuscated, \'rot13\')\n    msg_dict = json.loads(msg_str)\n    return cls(**msg_dict)\n\n    \nfrom fastkafka import FastKafka\n\nkafka_brokers = {\n    "localhost": {\n        "url": "localhost",\n        "description": "local development kafka broker",\n        "port": 9092,\n    },\n    "production": {\n        "url": "kafka.airt.ai",\n        "description": "production kafka broker",\n        "port": 9092,\n        "protocol": "kafka-secure",\n        "security": {"type": "plain"},\n    },\n}\n\nkafka_app = FastKafka(\n    title="Iris predictions",\n    kafka_brokers=kafka_brokers,\n    lifespan=lifespan,\n)\n\n@kafka_app.consumes(topic="input_data", decoder=custom_decoder)\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions", encoder=custom_encoder)\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,"This code defines a custom encoder and decoder functions for encoding\nand decoding messages sent through a Kafka messaging system."),(0,i.kt)("p",null,"The custom ",(0,i.kt)("inlineCode",{parentName:"p"},"encoder")," function takes a message represented as a\n",(0,i.kt)("inlineCode",{parentName:"p"},"BaseModel")," and encodes it as bytes by first converting it to a JSON\nstring and then obfuscating it using the ROT13 encoding. The obfuscated\nmessage is then converted to bytes using UTF-8 encoding and returned."),(0,i.kt)("p",null,"The custom ",(0,i.kt)("inlineCode",{parentName:"p"},"decoder")," function takes in the bytes representing an\nobfuscated message, decodes it using UTF-8 encoding, then decodes the\nROT13 obfuscation, and finally loads it as a dictionary using the ",(0,i.kt)("inlineCode",{parentName:"p"},"json"),"\nmodule. This dictionary is then converted to a ",(0,i.kt)("inlineCode",{parentName:"p"},"BaseModel")," instance\nusing the cls parameter."))}m.isMDXComponent=!0}}]);