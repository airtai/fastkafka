"use strict";(self.webpackChunkfastkafka=self.webpackChunkfastkafka||[]).push([[5845],{3905:(e,a,n)=>{n.d(a,{Zo:()=>c,kt:()=>m});var t=n(7294);function i(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function o(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),n.push.apply(n,t)}return n}function r(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?o(Object(n),!0).forEach((function(a){i(e,a,n[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))}))}return e}function s(e,a){if(null==e)return{};var n,t,i=function(e,a){if(null==e)return{};var n,t,i={},o=Object.keys(e);for(t=0;t<o.length;t++)n=o[t],a.indexOf(n)>=0||(i[n]=e[n]);return i}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(t=0;t<o.length;t++)n=o[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var p=t.createContext({}),l=function(e){var a=t.useContext(p),n=a;return e&&(n="function"==typeof e?e(a):r(r({},a),e)),n},c=function(e){var a=l(e.components);return t.createElement(p.Provider,{value:a},e.children)},d="mdxType",k={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},u=t.forwardRef((function(e,a){var n=e.components,i=e.mdxType,o=e.originalType,p=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=l(n),u=i,m=d["".concat(p,".").concat(u)]||d[u]||k[u]||o;return n?t.createElement(m,r(r({ref:a},c),{},{components:n})):t.createElement(m,r({ref:a},c))}));function m(e,a){var n=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=u;var s={};for(var p in a)hasOwnProperty.call(a,p)&&(s[p]=a[p]);s.originalType=e,s[d]="string"==typeof e?e:i,r[1]=s;for(var l=2;l<o;l++)r[l]=n[l];return t.createElement.apply(null,r)}return t.createElement.apply(null,n)}u.displayName="MDXCreateElement"},9017:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>p,contentTitle:()=>r,default:()=>k,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var t=n(7462),i=(n(7294),n(3905));const o={},r="Benchmarking FastKafka app",s={unversionedId:"guides/Guide_06_Benchmarking_FastKafka",id:"version-0.7.0/guides/Guide_06_Benchmarking_FastKafka",title:"Benchmarking FastKafka app",description:"Prerequisites",source:"@site/versioned_docs/version-0.7.0/guides/Guide_06_Benchmarking_FastKafka.md",sourceDirName:"guides",slug:"/guides/Guide_06_Benchmarking_FastKafka",permalink:"/docs/0.7.0/guides/Guide_06_Benchmarking_FastKafka",draft:!1,tags:[],version:"0.7.0",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Using FastAPI to Run FastKafka Application",permalink:"/docs/0.7.0/guides/Guide_32_Using_fastapi_to_run_fastkafka_application"},next:{title:"EventMetadata",permalink:"/docs/0.7.0/api/fastkafka/EventMetadata"}},p={},l=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Creating FastKafka Code",id:"creating-fastkafka-code",level:3},{value:"Starting Kafka",id:"starting-kafka",level:3},{value:"Installing Java and Kafka",id:"installing-java-and-kafka",level:4},{value:"Creating configuration for Zookeeper and Kafka",id:"creating-configuration-for-zookeeper-and-kafka",level:4},{value:"Starting Zookeeper and Kafka",id:"starting-zookeeper-and-kafka",level:4},{value:"Creating topics in Kafka",id:"creating-topics-in-kafka",level:4},{value:"Populating topics with dummy data",id:"populating-topics-with-dummy-data",level:4},{value:"Benchmarking FastKafka",id:"benchmarking-fastkafka",level:3}],c={toc:l},d="wrapper";function k(e){let{components:a,...n}=e;return(0,i.kt)(d,(0,t.Z)({},c,n,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"benchmarking-fastkafka-app"},"Benchmarking FastKafka app"),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("p",null,"To benchmark a ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," project, you will need the following:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"A library built with ",(0,i.kt)("inlineCode",{parentName:"li"},"FastKafka"),"."),(0,i.kt)("li",{parentName:"ol"},"A running ",(0,i.kt)("inlineCode",{parentName:"li"},"Kafka")," instance to benchmark the FastKafka application\nagainst.")),(0,i.kt)("h3",{id:"creating-fastkafka-code"},"Creating FastKafka Code"),(0,i.kt)("p",null,"Let\u2019s create a ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka"),"-based application and write it to the\n",(0,i.kt)("inlineCode",{parentName:"p"},"application.py")," file based on the ",(0,i.kt)("a",{parentName:"p",href:"/docs#tutorial"},"tutorial"),"."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# content of the "application.py" file\n\nfrom contextlib import asynccontextmanager\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nfrom fastkafka import FastKafka\n\nml_models = {}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastKafka):\n    # Load the ML model\n    X, y = load_iris(return_X_y=True)\n    ml_models["iris_predictor"] = LogisticRegression(random_state=0, max_iter=500).fit(\n        X, y\n    )\n    yield\n    # Clean up the ML models and release the resources\n    ml_models.clear()\n\n\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n\nfrom fastkafka import FastKafka\n\nkafka_brokers = {\n    "localhost": {\n        "url": "localhost",\n        "description": "local development kafka broker",\n        "port": 9092,\n    },\n    "production": {\n        "url": "kafka.airt.ai",\n        "description": "production kafka broker",\n        "port": 9092,\n        "protocol": "kafka-secure",\n        "security": {"type": "plain"},\n    },\n}\n\nkafka_app = FastKafka(\n    title="Iris predictions",\n    kafka_brokers=kafka_brokers,\n    lifespan=lifespan,\n)\n\n@kafka_app.consumes(topic="input_data", auto_offset_reset="latest")\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions")\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," has a decorator for benchmarking which is appropriately\ncalled as ",(0,i.kt)("inlineCode",{parentName:"p"},"benchmark"),". Let\u2019s edit our ",(0,i.kt)("inlineCode",{parentName:"p"},"application.py")," file and add the\n",(0,i.kt)("inlineCode",{parentName:"p"},"benchmark")," decorator to the consumes method."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# content of the "application.py" file with benchmark\n\nfrom contextlib import asynccontextmanager\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nfrom fastkafka import FastKafka\n\nml_models = {}\n\n\n@asynccontextmanager\nasync def lifespan(app: FastKafka):\n    # Load the ML model\n    X, y = load_iris(return_X_y=True)\n    ml_models["iris_predictor"] = LogisticRegression(random_state=0, max_iter=500).fit(\n        X, y\n    )\n    yield\n    # Clean up the ML models and release the resources\n    ml_models.clear()\n\n\nfrom pydantic import BaseModel, NonNegativeFloat, Field\n\nclass IrisInputData(BaseModel):\n    sepal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal length in cm"\n    )\n    sepal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Sepal width in cm"\n    )\n    petal_length: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal length in cm"\n    )\n    petal_width: NonNegativeFloat = Field(\n        ..., example=0.5, description="Petal width in cm"\n    )\n\n\nclass IrisPrediction(BaseModel):\n    species: str = Field(..., example="setosa", description="Predicted species")\n\nfrom fastkafka import FastKafka\n\nkafka_brokers = {\n    "localhost": {\n        "url": "localhost",\n        "description": "local development kafka broker",\n        "port": 9092,\n    },\n    "production": {\n        "url": "kafka.airt.ai",\n        "description": "production kafka broker",\n        "port": 9092,\n        "protocol": "kafka-secure",\n        "security": {"type": "plain"},\n    },\n}\n\nkafka_app = FastKafka(\n    title="Iris predictions",\n    kafka_brokers=kafka_brokers,\n    lifespan=lifespan,\n)\n\n@kafka_app.consumes(topic="input_data", auto_offset_reset="latest")\n@kafka_app.benchmark(interval=1, sliding_window_size=5)\nasync def on_input_data(msg: IrisInputData):\n    species_class = ml_models["iris_predictor"].predict(\n        [[msg.sepal_length, msg.sepal_width, msg.petal_length, msg.petal_width]]\n    )[0]\n\n    await to_predictions(species_class)\n\n\n@kafka_app.produces(topic="predictions")\nasync def to_predictions(species_class: int) -> IrisPrediction:\n    iris_species = ["setosa", "versicolor", "virginica"]\n\n    prediction = IrisPrediction(species=iris_species[species_class])\n    return prediction\n')),(0,i.kt)("p",null,"Here we are conducting a benchmark of a function that consumes data from\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"input_data")," topic with an interval of 1 second and a sliding window\nsize of 5."),(0,i.kt)("p",null,"This ",(0,i.kt)("inlineCode",{parentName:"p"},"benchmark")," method uses the ",(0,i.kt)("inlineCode",{parentName:"p"},"interval")," parameter to calculate the\nresults over a specific time period, and the ",(0,i.kt)("inlineCode",{parentName:"p"},"sliding_window_size"),"\nparameter to determine the maximum number of results to use in\ncalculating the average throughput and standard deviation."),(0,i.kt)("p",null,"This benchmark is important to ensure that the function is performing\noptimally and to identify any areas for improvement."),(0,i.kt)("h3",{id:"starting-kafka"},"Starting Kafka"),(0,i.kt)("p",null,"If you already have a ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," running somewhere, then you can skip this\nstep."),(0,i.kt)("p",null,"Please keep in mind that your benchmarking results may be affected by\nbottlenecks such as network, CPU cores in the Kafka machine, or even the\nKafka configuration itself."),(0,i.kt)("h4",{id:"installing-java-and-kafka"},"Installing Java and Kafka"),(0,i.kt)("p",null,"We need a working ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka"),"instance to benchmark our ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," app, and\nto run ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," we need ",(0,i.kt)("inlineCode",{parentName:"p"},"Java"),". Thankfully, ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," comes with a CLI\nto install both ",(0,i.kt)("inlineCode",{parentName:"p"},"Java")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," on our machine."),(0,i.kt)("p",null,"So, let\u2019s install ",(0,i.kt)("inlineCode",{parentName:"p"},"Java")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," by executing the following command."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"fastkafka testing install_deps\n")),(0,i.kt)("p",null,"The above command will extract ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka"),' scripts at the location\n\u201c\\$HOME/.local/kafka_2.13-3.3.2" on your machine.'),(0,i.kt)("h4",{id:"creating-configuration-for-zookeeper-and-kafka"},"Creating configuration for Zookeeper and Kafka"),(0,i.kt)("p",null,"Now we need to start ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," separately, and to start\nthem we need ",(0,i.kt)("inlineCode",{parentName:"p"},"zookeeper.properties")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"kafka.properties")," files."),(0,i.kt)("p",null,"Let\u2019s create a folder inside the folder where ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," scripts were\nextracted and change directory into it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"mkdir $HOME/.local/kafka_2.13-3.3.2/data_dir && cd $HOME/.local/kafka_2.13-3.3.2/data_dir\n")),(0,i.kt)("p",null,"Let\u2019s create a file called ",(0,i.kt)("inlineCode",{parentName:"p"},"zookeeper.properties")," and write the\nfollowing content to the file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},"dataDir=$HOME/.local/kafka_2.13-3.3.2/data_dir/zookeeper\nclientPort=2181\nmaxClientCnxns=0\n")),(0,i.kt)("p",null,"Similarly, let\u2019s create a file called ",(0,i.kt)("inlineCode",{parentName:"p"},"kafka.properties")," and write the\nfollowing content to the file:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},"broker.id=0\nlisteners=PLAINTEXT://:9092\n\nnum.network.threads=3\nnum.io.threads=8\nsocket.send.buffer.bytes=102400\nsocket.receive.buffer.bytes=102400\nsocket.request.max.bytes=104857600\n\nnum.partitions=1\nnum.recovery.threads.per.data.dir=1\noffsets.topic.replication.factor=1\ntransaction.state.log.replication.factor=1\ntransaction.state.log.min.isr=1\n\nlog.dirs=$HOME/.local/kafka_2.13-3.3.2/data_dir/kafka_logs\nlog.flush.interval.messages=10000\nlog.flush.interval.ms=1000\nlog.retention.hours=168\nlog.retention.bytes=1073741824\nlog.segment.bytes=1073741824\nlog.retention.check.interval.ms=300000\n\nzookeeper.connect=localhost:2181\nzookeeper.connection.timeout.ms=18000\n")),(0,i.kt)("h4",{id:"starting-zookeeper-and-kafka"},"Starting Zookeeper and Kafka"),(0,i.kt)("p",null,"We need two different terminals to run ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper")," in one and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," in\nanother. Let\u2019s open a new terminal and run the following commands to\nstart ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"export PATH=$PATH:$HOME/.jdk/jdk-11.0.18+10/bin\ncd $HOME/.local/kafka_2.13-3.3.2/bin\n./zookeeper-server-start.sh ../data_dir/zookeeper.properties\n")),(0,i.kt)("p",null,"Once ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper")," is up and running, open a new terminal and execute the\nfollwing commands to start ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"export PATH=$PATH:$HOME/.jdk/jdk-11.0.18+10/bin\ncd $HOME/.local/kafka_2.13-3.3.2/bin\n./kafka-server-start.sh ../data_dir/kafka.properties\n")),(0,i.kt)("p",null,"Now we have both ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," up and running."),(0,i.kt)("h4",{id:"creating-topics-in-kafka"},"Creating topics in Kafka"),(0,i.kt)("p",null,"In a new terminal, please execute the following command to create\nnecessary topics in ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"export PATH=$PATH:$HOME/.jdk/jdk-11.0.18+10/bin\ncd $HOME/.local/kafka_2.13-3.3.2/bin\n./kafka-topics.sh --create --topic input_data --partitions 6 --bootstrap-server localhost:9092\n./kafka-topics.sh --create --topic predictions --partitions 6 --bootstrap-server localhost:9092\n")),(0,i.kt)("h4",{id:"populating-topics-with-dummy-data"},"Populating topics with dummy data"),(0,i.kt)("p",null,"To benchmark our ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," app, we need some data in ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," topics."),(0,i.kt)("p",null,"In the same terminal, let\u2019s create some dummy data:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},'yes \'{"sepal_length": 0.7739560486, "sepal_width": 0.8636615789, "petal_length": 0.6122663046, "petal_width": 0.1338914722}\' | head -n 1000000 > /tmp/test_data\n')),(0,i.kt)("p",null,"This command will create a file called ",(0,i.kt)("inlineCode",{parentName:"p"},"test_data")," in the ",(0,i.kt)("inlineCode",{parentName:"p"},"tmp")," folder\nwith one million rows of text. This will act as dummy data to populate\nthe ",(0,i.kt)("inlineCode",{parentName:"p"},"input_data")," topic."),(0,i.kt)("p",null,"Let\u2019s populate the created topic ",(0,i.kt)("inlineCode",{parentName:"p"},"input_data")," with the dummy data which\nwe created above:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic input_data < /tmp/test_data\n")),(0,i.kt)("p",null,"Now our topic ",(0,i.kt)("inlineCode",{parentName:"p"},"input_data")," has one million records/messages in it. If\nyou want more messages in topic, you can simply execute the above\ncommand again and again."),(0,i.kt)("h3",{id:"benchmarking-fastkafka"},"Benchmarking FastKafka"),(0,i.kt)("p",null,"Once ",(0,i.kt)("inlineCode",{parentName:"p"},"Zookeeper")," and ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka")," are ready, benchmarking ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," app is\nas simple as running the ",(0,i.kt)("inlineCode",{parentName:"p"},"fastkafka run")," command:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-cmd"},"fastkafka run --num-workers 1 --kafka-broker localhost application:kafka_app\n")),(0,i.kt)("p",null,"This command will start the ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," app and begin consuming messages\nfrom ",(0,i.kt)("inlineCode",{parentName:"p"},"Kafka"),", which we spun up earlier. Additionally, the same command\nwill output all of the benchmark throughputs based on the ",(0,i.kt)("inlineCode",{parentName:"p"},"interval")," and\n",(0,i.kt)("inlineCode",{parentName:"p"},"sliding_window_size")," values."),(0,i.kt)("p",null,"The output for the ",(0,i.kt)("inlineCode",{parentName:"p"},"fastkafka run")," command is:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},"[385814]: 23-04-07 10:49:18.380 [INFO] application: Current group id is ZDGTBVWVBBDMZCW\n[385814]: 23-04-07 10:49:18.382 [INFO] fastkafka._application.app: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'\n[385814]: 23-04-07 10:49:18.382 [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': 'localhost:9092'}'\n[385814]: 23-04-07 10:49:18.387 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...\n[385814]: 23-04-07 10:49:18.387 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'group_id': 'ZDGTBVWVBBDMZCW', 'auto_offset_reset': 'earliest', 'bootstrap_servers': 'localh\nost:9092', 'max_poll_records': 100}\n[385814]: 23-04-07 10:49:18.390 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.\n[385814]: 23-04-07 10:49:18.390 [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'input_data'})\n[385814]: 23-04-07 10:49:18.390 [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'input_data'}\n[385814]: 23-04-07 10:49:18.390 [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.\n[385814]: 23-04-07 10:49:18.395 [INFO] aiokafka.consumer.group_coordinator: Discovered coordinator 0 for group ZDGTBVWVBBDMZCW\n[385814]: 23-04-07 10:49:18.395 [INFO] aiokafka.consumer.group_coordinator: Revoking previously assigned partitions set() for group ZDGTBVWVBBDMZCW\n[385814]: 23-04-07 10:49:18.395 [INFO] aiokafka.consumer.group_coordinator: (Re-)joining group ZDGTBVWVBBDMZCW\n[385814]: 23-04-07 10:49:21.396 [INFO] aiokafka.consumer.group_coordinator: Joined group 'ZDGTBVWVBBDMZCW' (generation 1) with member_id aiokafka-0.8.0-b1f06560-6983-4d5e-a9af-8084e0e652cc\n[385814]: 23-04-07 10:49:21.396 [INFO] aiokafka.consumer.group_coordinator: Elected group leader -- performing partition assignments using roundrobin\n[385814]: 23-04-07 10:49:21.397 [INFO] aiokafka.consumer.group_coordinator: Successfully synced group ZDGTBVWVBBDMZCW with generation 1\n[385814]: 23-04-07 10:49:21.397 [INFO] aiokafka.consumer.group_coordinator: Setting newly assigned partitions {TopicPartition(topic='input_data', partition=0), TopicPartition(topic='input_data', partition=1), TopicPartition(topic='input_data', partition\n=2), TopicPartition(topic='input_data', partition=3)} for group ZDGTBVWVBBDMZCW\n[385814]: 23-04-07 10:49:22.409 [INFO] fastkafka.benchmark: Throughput = 93,598, Avg throughput = 93,598 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:23.409 [INFO] fastkafka.benchmark: Throughput = 91,847, Avg throughput = 92,723 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:24.409 [INFO] fastkafka.benchmark: Throughput = 92,948, Avg throughput = 92,798 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:25.409 [INFO] fastkafka.benchmark: Throughput = 93,227, Avg throughput = 92,905 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:26.409 [INFO] fastkafka.benchmark: Throughput = 93,553, Avg throughput = 93,035 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:27.409 [INFO] fastkafka.benchmark: Throughput = 92,699, Avg throughput = 92,855 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:28.409 [INFO] fastkafka.benchmark: Throughput = 92,716, Avg throughput = 93,029 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:29.409 [INFO] fastkafka.benchmark: Throughput = 92,897, Avg throughput = 93,019 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:30.409 [INFO] fastkafka.benchmark: Throughput = 92,854, Avg throughput = 92,944 - For application.on_input_data(interval=1,sliding_window_size=5)\n[385814]: 23-04-07 10:49:31.410 [INFO] fastkafka.benchmark: Throughput = 92,672, Avg throughput = 92,768 - For application.on_input_data(interval=1,sliding_window_size=5)\n")),(0,i.kt)("p",null,"Based on the output, when using 1 worker, our ",(0,i.kt)("inlineCode",{parentName:"p"},"FastKafka")," app achieved a\n",(0,i.kt)("inlineCode",{parentName:"p"},"throughput")," of 93k messages per second and an ",(0,i.kt)("inlineCode",{parentName:"p"},"average throughput")," of\n93k messages per second."))}k.isMDXComponent=!0}}]);