Docusaurus Helper
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
from tempfile import TemporaryDirectory

import pytest
from pydantic import BaseModel
```

``` python
fixture = parse(
    """
This is a docstring for a sample function.

It can contain multiple lines and can include *markdown* syntax.

Args:
    name: name of the person
    age: age of the person
    
Returns:
    A formatted string

Raises:
    ValueError: If name is not a string
    TypeError: If name is not a string
"""
)

actual = _format_docstring_sections(fixture.params, "Parameters")
expected = """**Parameters**:
- `name`: name of the person
- `age`: age of the person

"""

print(actual)
assert actual == expected
```

    **Parameters**:
    - `name`: name of the person
    - `age`: age of the person

``` python
actual = _format_docstring_sections(fixture.many_returns, "Returns")
expected = """**Returns**:
- A formatted string

"""

print(actual)
assert actual == expected
```

    **Returns**:
    - A formatted string

``` python
actual = _format_docstring_sections(fixture.raises, "Exceptions")
expected = """**Exceptions**:
- `ValueError`: If name is not a string
- `TypeError`: If name is not a string

"""

print(actual)
assert actual == expected
```

    **Exceptions**:
    - `ValueError`: If name is not a string
    - `TypeError`: If name is not a string

``` python
fixture = """This is a docstring for a sample function."""

expected = """This is a docstring for a sample function.

"""

actual = _docstring_to_markdown(fixture)
print(actual)

assert actual == expected
```

    This is a docstring for a sample function.

``` python
fixture = """This is a docstring for a sample function.

Args:
    name: name of the person
    age: age of the person
"""

expected = """This is a docstring for a sample function.

**Parameters**:
- `name`: name of the person
- `age`: age of the person

"""

actual = _docstring_to_markdown(fixture)
print(actual)

assert actual == expected
```

    This is a docstring for a sample function.

    **Parameters**:
    - `name`: name of the person
    - `age`: age of the person

``` python
fixture = """
This is a docstring for a sample function.

It can contain multiple lines and can include *markdown* syntax.

Args:
    name: name of the person
    age: age of the person
    
Returns:
    A formatted string

Raises:
    ValueError: If name is not a string
    TypeError: If name is not a string
"""

expected = """This is a docstring for a sample function.

It can contain multiple lines and can include *markdown* syntax.

**Parameters**:
- `name`: name of the person
- `age`: age of the person

**Returns**:
- A formatted string

**Exceptions**:
- `ValueError`: If name is not a string
- `TypeError`: If name is not a string

"""

actual = _docstring_to_markdown(fixture)
print(actual)

assert actual == expected
```

    This is a docstring for a sample function.

    It can contain multiple lines and can include *markdown* syntax.

    **Parameters**:
    - `name`: name of the person
    - `age`: age of the person

    **Returns**:
    - A formatted string

    **Exceptions**:
    - `ValueError`: If name is not a string
    - `TypeError`: If name is not a string

``` python
module_name = "fastkafka"
members_with_submodules = _get_submodules(module_name)
members_with_submodules
```

    ['fastkafka',
     'fastkafka.EventMetadata',
     'fastkafka.FastKafka',
     'fastkafka.KafkaEvent',
     'fastkafka.testing',
     'fastkafka.testing.ApacheKafkaBroker',
     'fastkafka.testing.LocalRedpandaBroker',
     'fastkafka.testing.Tester']

``` python
module_name = "fastkafka"
members_with_submodules = _get_submodules(module_name)
symbols = _load_submodules(module_name, members_with_submodules)
symbols
```

    [fastkafka.EventMetadata,
     fastkafka.FastKafka,
     fastkafka.KafkaEvent,
     fastkafka.testing.ApacheKafkaBroker,
     fastkafka.testing.LocalRedpandaBroker,
     fastkafka.testing.Tester]

``` python
fixtures = [
    {
        "input": "arg_1: Union[int, NoneType] = 80",
        "expected": "arg_1: Optional[int] = 80",
    },
    {
        "input": "arg_1: Union[Dict[str, str], NoneType]",
        "expected": "arg_1: Optional[Dict[str, str]]",
    },
    {
        "input": "arg_1: Union[Dict[str, str], str]",
        "expected": "arg_1: Union[Dict[str, str], str]",
    },
    {
        "input": "arg_1: str",
        "expected": "arg_1: str",
    },
    {
        "input": "arg_1: bool = False",
        "expected": "arg_1: bool = False",
    },
    {
        "input": "prefix: str = 'to_'",
        "expected": "prefix: str = 'to_'",
    },
]

for fixture in fixtures:
    actual = _convert_union_to_optional(fixture["input"])
    print(actual)
    assert actual == fixture["expected"]
```

    arg_1: Optional[int] = 80
    arg_1: Optional[Dict[str, str]]
    arg_1: Union[Dict[str, str], str]
    arg_1: str
    arg_1: bool = False
    prefix: str = 'to_'

``` python
def fixture_function(
    arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80
) -> str:
    pass


_signature = signature(fixture_function)

expected = (
    "arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80"
)
actual = _get_arg_list_with_signature(_signature)

print(actual)
assert actual == expected
```

    arg_1: str, arg_2, arg_3: Union[Dict[str, str], str], arg_4: Optional[int] = 80

``` python
def fixture_function(arg_1: str, arg_2) -> None:
    pass


_signature = signature(fixture_function)

expected = "arg_1: str, arg_2"
actual = _get_arg_list_with_signature(_signature)

print(actual)
assert actual == expected
```

    arg_1: str, arg_2

``` python
TestCallable = Callable[[BaseModel], Union[Awaitable[None], None]]


def fixture_function(arg_1: str) -> TestCallable:
    pass


actual = _get_symbol_definition(fixture_function)
print(actual)
assert (
    "`def fixture_function(arg_1: str) -> typing.Callable[[pydantic.main.BaseModel]"
    in actual
)
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: str) -> typing.Callable[[pydantic.main.BaseModel], typing.Optional[typing.Awaitable[NoneType]]]`

``` python
def fixture_function(arg_1: str, arg_2) -> None:
    pass


actual = _get_symbol_definition(fixture_function)
expected = "### `fixture_function` {#fixture_function}\n\n`def fixture_function(arg_1: str, arg_2) -> None`\n"

print(actual)
assert actual == expected
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: str, arg_2) -> None`

``` python
def fixture_function(arg_1: str, arg_2) -> int:
    pass


actual = _get_symbol_definition(fixture_function)
expected = "### `fixture_function` {#fixture_function}\n\n`def fixture_function(arg_1: str, arg_2) -> int`\n"

print(actual)
assert actual == expected
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: str, arg_2) -> int`

``` python
def fixture_function(arg_1: str, arg_2) -> "Tester":
    pass


actual = _get_symbol_definition(fixture_function)
expected = "### `fixture_function` {#fixture_function}\n\n`def fixture_function(arg_1: str, arg_2) -> Tester`\n"

print(actual)
assert actual == expected
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: str, arg_2) -> Tester`

``` python
def __fixture_function__(arg_1: str, arg_2) -> "Tester":
    pass


actual = _get_symbol_definition(__fixture_function__)
expected = "### `__fixture_function__` {#fixture_function}\n\n`def __fixture_function__(arg_1: str, arg_2) -> Tester`\n"

print(actual)
assert actual == expected
```

    ### `__fixture_function__` {#fixture_function}

    `def __fixture_function__(arg_1: str, arg_2) -> Tester`

``` python
def fixture_function(
    arg_1: int,
    arg_2: str = "default_string",
    arg_3: Dict[str, int] = {},
    arg_4: Optional[float] = None,
    arg_5: Tuple[int, str, float] = (1, "string", 2.0),
    arg_6: List[Union[int, str]] = [1, "string"],
    arg_7: Set[int] = {1, 2, 3},
    arg_8: Union[int, str] = "string",
) -> None:
    pass


actual = _get_symbol_definition(fixture_function)
expected = "### `fixture_function` {#fixture_function}\n\n`def fixture_function(arg_1: int, arg_2: str = 'default_string', arg_3: Dict[str, int] = {}, arg_4: Optional[float] = None, arg_5: Tuple[int, str, float] = (1, 'string', 2.0), arg_6: List[Union[int, str]] = [1, 'string'], arg_7: Set[int] = {1, 2, 3}, arg_8: Union[int, str] = 'string') -> None`\n"

print(actual)
assert actual == expected
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: int, arg_2: str = 'default_string', arg_3: Dict[str, int] = {}, arg_4: Optional[float] = None, arg_5: Tuple[int, str, float] = (1, 'string', 2.0), arg_6: List[Union[int, str]] = [1, 'string'], arg_7: Set[int] = {1, 2, 3}, arg_8: Union[int, str] = 'string') -> None`

``` python
def fixture_function(
    arg_1: str,
    arg_2: Union[List[str], str],
    arg_3: Optional[int],
    arg_4: Optional[str] = None,
) -> str:
    """This is a one line description for the function

    Args:
        arg_1: Argument 1
        arg_2: Argument 2
        arg_3: Argument 3
        arg_4: Argument 4

    Returns:
        The concatinated string
    """
    pass


expected = """### `fixture_function` {#fixture_function}\n\n`def fixture_function(arg_1: str, arg_2: Union[List[str], str], arg_3: Optional[int], arg_4: Optional[str] = None) -> str`

This is a one line description for the function

**Parameters**:
- `arg_1`: Argument 1
- `arg_2`: Argument 2
- `arg_3`: Argument 3
- `arg_4`: Argument 4

**Returns**:
- The concatinated string

"""

actual = _get_formatted_docstring_for_symbol(fixture_function)
print(actual)

assert actual == expected
```

    ### `fixture_function` {#fixture_function}

    `def fixture_function(arg_1: str, arg_2: Union[List[str], str], arg_3: Optional[int], arg_4: Optional[str] = None) -> str`

    This is a one line description for the function

    **Parameters**:
    - `arg_1`: Argument 1
    - `arg_2`: Argument 2
    - `arg_3`: Argument 3
    - `arg_4`: Argument 4

    **Returns**:
    - The concatinated string

``` python
class Vehicle:
    """This is a docstring for the class"""

    def __init__(self, brand: str, model: str, type: str):
        """Constructor

        Args:
            brand: Name of the brand
            model: Name of the model
            type: Model type
        """
        self.brand = brand
        self.model = model
        self.type = type
        self.gas_tank_size = 14
        self.fuel_level = 0

    def fuel_up(self):
        """Fuel up"""
        self.fuel_level = self.gas_tank_size
        print("Gas tank is now full.")

    def drive(self):
        """Drive"""
        print(f"The {self.model} is now driving.")


expected = """
This is a docstring for the class

### `__init__` {#init}

`def __init__(self, brand: str, model: str, type: str) -> None`

Constructor

**Parameters**:
- `brand`: Name of the brand
- `model`: Name of the model
- `type`: Model type

### `drive` {#drive}

`def drive(self) -> None`

Drive

### `fuel_up` {#fuel_up}

`def fuel_up(self) -> None`

Fuel up

"""

actual = _get_formatted_docstring_for_symbol(Vehicle)
print(actual)

assert actual == expected
```


    This is a docstring for the class

    ### `__init__` {#init}

    `def __init__(self, brand: str, model: str, type: str) -> None`

    Constructor

    **Parameters**:
    - `brand`: Name of the brand
    - `model`: Name of the model
    - `type`: Model type

    ### `drive` {#drive}

    `def drive(self) -> None`

    Drive

    ### `fuel_up` {#fuel_up}

    `def fuel_up(self) -> None`

    Fuel up

``` python
class Outer:
    """Outer Class"""

    def __init__(self):
        """Outer class constructor"""
        ## instantiating the 'Inner' class
        self.inner = self.Inner()

    def reveal(self):
        """Reveal function"""
        ## calling the 'Inner' class function display
        self.inner.inner_display("Calling Inner class function from Outer class")

    class Inner:
        """Inner Class"""

        def inner_display(self, msg):
            """Inner display"""
            print(msg)

    class Inner2:
        """Inner2 Class"""

        def inner_display_2(self, msg):
            """Inner display_2"""
            print(msg)


expected = """
Outer Class


Inner Class

### `inner_display` {#inner_display}

`def inner_display(self, msg) -> None`

Inner display


Inner2 Class

### `inner_display_2` {#inner_display_2}

`def inner_display_2(self, msg) -> None`

Inner display_2

### `__init__` {#init}

`def __init__(self) -> None`

Outer class constructor

### `reveal` {#reveal}

`def reveal(self) -> None`

Reveal function

"""

actual = _get_formatted_docstring_for_symbol(Outer)
print(actual)

assert actual == expected
```


    Outer Class


    Inner Class

    ### `inner_display` {#inner_display}

    `def inner_display(self, msg) -> None`

    Inner display


    Inner2 Class

    ### `inner_display_2` {#inner_display_2}

    `def inner_display_2(self, msg) -> None`

    Inner display_2

    ### `__init__` {#init}

    `def __init__(self) -> None`

    Outer class constructor

    ### `reveal` {#reveal}

    `def reveal(self) -> None`

    Reveal function

``` python
fixtures = [
    {
        "input": """<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a> some text goes here <a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>""",
        "expected": """<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a> some text goes here <a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>""",
    },
    {
        "input": '<span style={{color: 'red'}}>Test</span>',
        "expected": "<span style={{color: 'red'}}>Test</span>",
    },
    {
        "input": '<div style={{backgroundColor: 'blue', border: '1px solid black'}}>Test</div>',
        "expected": "<div style={{backgroundColor: 'blue', border: '1px solid black'}}>Test</div>",
    },
    {
        "input": '<span style={{fontSize: '1.2rem'}}>Test</span>',
        "expected": "<span style={{fontSize: '1.2rem'}}>Test</span>",
    },
    {
        "input": """<pre style={{whiteSpace: 'pre', overflowX: 'auto', lineHeight: 'normal', fontFamily: 'Menlo,"DejaVu Sans Mono",consolas,"Courier New",monospace'}}></pre>""",
        "expected": """<pre style={{whiteSpace: 'pre', overflowX: 'auto', lineHeight: 'normal', fontFamily: 'Menlo,"DejaVu Sans Mono",consolas,"Courier New",monospace'}}></pre>""",
    },
]

for fixture in fixtures:
    actual = _convert_html_style_attribute_to_jsx(fixture["input"])
    print("*" * 120)
    print(actual)
    assert actual == fixture["expected"], fixture["expected"]
```

    ************************************************************************************************************************
    <a
    href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
    target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a> some text goes here <a
    href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_components/test_dependencies.py#L28"
    target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>
    ************************************************************************************************************************
    <span style={{color: 'red'}}>Test</span>
    ************************************************************************************************************************
    <div style={{backgroundColor: 'blue', border: '1px solid black'}}>Test</div>
    ************************************************************************************************************************
    <span style={{fontSize: '1.2rem'}}>Test</span>
    ************************************************************************************************************************
    <pre style={{whiteSpace: 'pre', overflowX: 'auto', lineHeight: 'normal', fontFamily: 'Menlo,"DejaVu Sans Mono",consolas,"Courier New",monospace'}}></pre>

``` python
with TemporaryDirectory() as d:
    module_name = "fastkafka"

    docs_path = Path(d) / "docusaurus" / "docs"
    docs_path.mkdir(parents=True)

    api_path = docs_path / "api"
    api_path.mkdir(parents=True)

    blog_path = docs_path / "blog"
    blog_path.mkdir(parents=True)

    nested_api_path = api_path / "fastKafka"
    nested_api_path.mkdir(parents=True)

    for p in [docs_path, api_path, blog_path, nested_api_path]:
        with open((p / "file.md"), "w") as f:
            f.write("sample text")

    actual = _get_all_markdown_files_path(docs_path)
    expected = [
        Path(docs_path) / "file.md",
        Path(api_path) / "file.md",
        Path(nested_api_path) / "file.md",
        Path(blog_path) / "file.md",
    ]

    print(actual)
    assert sorted(actual) == sorted(expected), expected
```

    [Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmp_gvhhlvl/docusaurus/docs/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmp_gvhhlvl/docusaurus/docs/blog/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmp_gvhhlvl/docusaurus/docs/api/file.md'), Path('/var/folders/6n/3rjds7v52cd83wqkd565db0h0000gn/T/tmp_gvhhlvl/docusaurus/docs/api/fastKafka/file.md')]

``` python
fixture = """<a href="https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb" target="_blank">"""
expected = """<a href="https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb" target="_blank">"""

actual = _fix_special_symbols_in_html(fixture)
print(actual)
assert actual == expected
```

    <a href="https://colab.research.google.com/github/airtai/fastkafka/blob/main/nbs/guides/Guide_00_FastKafka_Demo.ipynb" target="_blank">

``` python
fixture = "./api/fastkafka/FastKafka.md/#fastkafka.FastKafka"
expected = "https://airtai.github.io/fastkafka/api/fastkafka/FastKafka.md/#fastkafka.FastKafka"

actual = _add_file_extension_to_link(fixture)
print(actual)
assert actual == expected
```

    https://airtai.github.io/fastkafka/api/fastkafka/FastKafka.md/#fastkafka.FastKafka

``` python
fixture = "./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker"
expected = "https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker"

actual = _add_file_extension_to_link(fixture)
print(actual)
assert actual == expected
```

    https://airtai.github.io/fastkafka/api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker

``` python
fixture = "https://github.com/airtai/sample_fastkafka_with_redpanda"
expected = "https://github.com/airtai/sample_fastkafka_with_redpanda.md"

actual = _add_file_extension_to_link(fixture)
print(actual)
assert actual == expected
```

    https://github.com/airtai/sample_fastkafka_with_redpanda.md

``` python
fixture = """In the above example,
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
app is named as `kafka_app`
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)"""

expected = """In the above example,
[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
app is named as `kafka_app`
[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)"""

dir_prefix = "../"
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    In the above example,
    [`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    app is named as `kafka_app`
    [`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)

``` python
fixture = """In the above example,
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
app is named as `kafka_app`
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)"""

expected = """In the above example,
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
app is named as `kafka_app`
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)"""

dir_prefix = ""
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    In the above example,
    [`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    app is named as `kafka_app`
    [`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)

``` python
fixture = """The service can be tested using the
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
instance and we can start the Kafka
broker locally using the
[`ApacheKafkaBroker`](./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker)."""

expected = """The service can be tested using the
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
instance and we can start the Kafka
broker locally using the
[`ApacheKafkaBroker`](./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker)."""

dir_prefix = ""
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    The service can be tested using the
    [`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    instance and we can start the Kafka
    broker locally using the
    [`ApacheKafkaBroker`](./api/fastkafka/testing/ApacheKafkaBroker.md/#fastkafka.testing.ApacheKafkaBroker).

``` python
fixture = """This is not a link to a symbol: https://www.google.com"""

expected = """This is not a link to a symbol: https://www.google.com"""

dir_prefix = ""
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    This is not a link to a symbol: https://www.google.com

``` python
fixture = """A sample fastkafka-based library that uses Redpanda for testing, based
on this guide, can be found
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)"""

expected = """A sample fastkafka-based library that uses Redpanda for testing, based
on this guide, can be found
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)"""

dir_prefix = ""
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    A sample fastkafka-based library that uses Redpanda for testing, based
    on this guide, can be found
    [here](https://github.com/airtai/sample_fastkafka_with_redpanda)

``` python
fixture = """To learn more about Redpanda, please visit their
[website](https://redpanda.com/) or checkout this [blog
post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)
comparing Redpanda and Kafka’s performance benchmarks."""

expected = """To learn more about Redpanda, please visit their
[website](https://redpanda.com/) or checkout this [blog
post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)
comparing Redpanda and Kafka’s performance benchmarks."""

dir_prefix = ""
doc_host="https://airtai.github.io"
doc_baseurl="/fastkafka"
actual = _fix_symbol_links(fixture, dir_prefix, doc_host, doc_baseurl)
print(actual)
assert actual == expected
```

    To learn more about Redpanda, please visit their
    [website](https://redpanda.com/) or checkout this [blog
    post](https://redpanda.com/blog/redpanda-vs-kafka-performance-benchmark)
    comparing Redpanda and Kafka’s performance benchmarks.

``` python
docs_path = Path('docusaurus/docs')

sub_path = Path('docusaurus/docs/index.md')
actual = _get_relative_url_prefix(docs_path, sub_path) 
print(actual)
assert actual == ""

sub_path = Path('docusaurus/docs/guides/Guide_31_Using_redpanda_to_test_fastkafka.md')
actual = _get_relative_url_prefix(docs_path, sub_path)
print(actual)
assert actual == "../"

sub_path = Path('docusaurus/docs/guides/tutorial/fastkafka.md')
actual = _get_relative_url_prefix(docs_path, sub_path)
print(actual)
assert actual == "../../"

with pytest.raises(ValueError) as e:
    sub_path = Path('mkdocs/docs/guides/tutorial/fastkafka.md')
    _get_relative_url_prefix(docs_path, sub_path)
```


    ../
    ../../

------------------------------------------------------------------------

### fix_invalid_syntax_in_markdown

>      fix_invalid_syntax_in_markdown (docs_path:str)

Fix invalid HTML syntax in markdown files and converts inline style
attributes to JSX-compatible format.

Args: docs_path: The path to the root directory to search for markdown
files.

``` python
with TemporaryDirectory() as d:
    module_name = "fastkafka"

    docs_path = Path(d) / "docusaurus" / "docs"
    docs_path.mkdir(parents=True)

    api_path = docs_path / "api"
    api_path.mkdir(parents=True)

    blog_path = docs_path / "blog"
    blog_path.mkdir(parents=True)

    nested_api_path = api_path / "fastKafka"
    nested_api_path.mkdir(parents=True)

    for p in [docs_path, api_path, blog_path, nested_api_path]:
        with open((p / "file.md"), "w") as f:
            f.write(
                """source some text goes here Test and one more tag Test
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)
"""
            )

    fix_invalid_syntax_in_markdown(str(docs_path))
    expected = [
        """source some text goes here Test and one more tag Test
[`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)
""",
        """source some text goes here Test and one more tag Test
[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)
""",
        """source some text goes here Test and one more tag Test
[`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)
""",
        """source some text goes here Test and one more tag Test
[`FastKafka`](../../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
[`Tester`](../../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
[here](https://github.com/airtai/sample_fastkafka_with_redpanda)
""",
    ]

    for i, p in enumerate([docs_path, api_path, blog_path, nested_api_path]):
        with open((p / "file.md"), "r") as f:
            actual = f.read()
            print("*" * 120)
            print(actual)
            assert actual == expected[i]
```

    ************************************************************************************************************************
    source some text goes here Test and one more tag Test
    [`FastKafka`](./api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`Tester`](./api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [here](https://github.com/airtai/sample_fastkafka_with_redpanda)

    ************************************************************************************************************************
    source some text goes here Test and one more tag Test
    [`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [here](https://github.com/airtai/sample_fastkafka_with_redpanda)

    ************************************************************************************************************************
    source some text goes here Test and one more tag Test
    [`FastKafka`](../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`Tester`](../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [here](https://github.com/airtai/sample_fastkafka_with_redpanda)

    ************************************************************************************************************************
    source some text goes here Test and one more tag Test
    [`FastKafka`](../../api/fastkafka/FastKafka.md/#fastkafka.FastKafka)
    [`Tester`](../../api/fastkafka/testing/Tester.md/#fastkafka.testing.Tester)
    [here](https://github.com/airtai/sample_fastkafka_with_redpanda)

------------------------------------------------------------------------

### generate_markdown_docs

>      generate_markdown_docs (module_name:str, docs_path:str)

Generates Markdown documentation files for the symbols in the given
module and save them to the given directory.

Args: module_name: The name of the module to generate documentation for.
docs_path: The path to the directory where the documentation files will
be saved.

``` python
with TemporaryDirectory() as d:
    module_name = "fastkafka"

    docs_path = Path(d) / "docusaurus" / "docs"
    docs_path.mkdir(parents=True)

    api_path = docs_path / "api"
    api_path.mkdir(parents=True)

    members_with_submodules = _get_submodules(module_name)
    symbols = _load_submodules(module_name, members_with_submodules)
    for symbol in symbols:
        target_file_path = (
            "/".join(f"{symbol.__module__}.{symbol.__name__}".split(".")) + ".md"
        )
        (api_path / "/".join(f"{symbol.__module__}".split("."))).mkdir(
            parents=True, exist_ok=True
        )

        with open((api_path / target_file_path), "w") as f:
            f.write(f"Initial content in '{target_file_path}'")

        with open((api_path / target_file_path), "r") as f:
            contents = f.read()
            print(contents)
            assert f"Initial content in '{target_file_path}'" == contents, contents

    generate_markdown_docs(module_name, str(docs_path))

    print("*" * 100)
    for symbol in symbols:
        target_file_path = (
            "/".join(f"{symbol.__module__}.{symbol.__name__}".split(".")) + ".md"
        )
        (api_path / "/".join(f"{symbol.__module__}".split("."))).mkdir(
            parents=True, exist_ok=True
        )

        with open((api_path / target_file_path), "r") as f:
            contents = f.read()
            print(contents)
            assert f"Initial content in '{target_file_path}'" != contents, contents
```

    Initial content in 'fastkafka/EventMetadata.md'
    Initial content in 'fastkafka/FastKafka.md'
    Initial content in 'fastkafka/KafkaEvent.md'
    Initial content in 'fastkafka/testing/ApacheKafkaBroker.md'
    Initial content in 'fastkafka/testing/LocalRedpandaBroker.md'
    Initial content in 'fastkafka/testing/Tester.md'
    ****************************************************************************************************
    ## `fastkafka.EventMetadata` {#fastkafka.EventMetadata}


    A class for encapsulating Kafka record metadata.

    **Parameters**:
    - `topic`: The topic this record is received from
    - `partition`: The partition from which this record is received
    - `offset`: The position of this record in the corresponding Kafka partition
    - `timestamp`: The timestamp of this record
    - `timestamp_type`: The timestamp type of this record
    - `key`: The key (or `None` if no key is specified)
    - `value`: The value
    - `serialized_key_size`: The size of the serialized, uncompressed key in bytes
    - `serialized_value_size`: The size of the serialized, uncompressed value in bytes
    - `headers`: The headers


    ## `fastkafka.FastKafka` {#fastkafka.FastKafka}

    ### `__init__` {#init}

    `def __init__(self, title: Optional[str] = None, description: Optional[str] = None, version: Optional[str] = None, contact: Optional[Dict[str, str]] = None, kafka_brokers: Dict[str, Any], root_path: Optional[pathlib.Path, str] = None, lifespan: Optional[Callable[[ForwardRef('FastKafka')], AsyncContextManager[NoneType]]] = None, loop=None, client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None, group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, exclude_internal_topics=True, isolation_level='read_uncommitted') -> None`

    Creates FastKafka application

    **Parameters**:
    - `title`: optional title for the documentation. If None,
    the title will be set to empty string
    - `description`: optional description for the documentation. If
    None, the description will be set to empty string
    - `version`: optional version for the documentation. If None,
    the version will be set to empty string
    - `contact`: optional contact for the documentation. If None, the
    contact will be set to placeholder values:
    name='Author' url=HttpUrl(' https://www.google.com ', ) email='noreply@gmail.com'
    - `kafka_brokers`: dictionary describing kafka brokers used for
    generating documentation
    - `root_path`: path to where documentation will be created
    - `lifespan`: asynccontextmanager that is used for setting lifespan hooks.
    __aenter__ is called before app start and __aexit__ after app stop.
    The lifespan is called whe application is started as async context
    manager, e.g.:`async with kafka_app...`
    - `client_id`: a name for this client. This string is passed in
    each request to servers and can be used to identify specific
    server-side log entries that correspond to this client.
    Default: ``aiokafka-producer-#`` (appended with a unique number
    per instance)
    - `key_serializer`: used to convert user-supplied keys to bytes
    If not :data:`None`, called as ``f(key),`` should return
    :class:`bytes`.
    Default: :data:`None`.
    - `value_serializer`: used to convert user-supplied message
    values to :class:`bytes`. If not :data:`None`, called as
    ``f(value)``, should return :class:`bytes`.
    Default: :data:`None`.
    - `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments
    the producer requires the leader to have received before considering a
    request complete. This controls the durability of records that are
    sent. The following settings are common:

    * ``0``: Producer will not wait for any acknowledgment from the server
      at all. The message will immediately be added to the socket
      buffer and considered sent. No guarantee can be made that the
      server has received the record in this case, and the retries
      configuration will not take effect (as the client won't
      generally know of any failures). The offset given back for each
      record will always be set to -1.
    * ``1``: The broker leader will write the record to its local log but
      will respond without awaiting full acknowledgement from all
      followers. In this case should the leader fail immediately
      after acknowledging the record but before the followers have
      replicated it then the record will be lost.
    * ``all``: The broker leader will wait for the full set of in-sync
      replicas to acknowledge the record. This guarantees that the
      record will not be lost as long as at least one in-sync replica
      remains alive. This is the strongest available guarantee.

    If unset, defaults to ``acks=1``. If `enable_idempotence` is
    :data:`True` defaults to ``acks=all``
    - `compression_type`: The compression type for all data generated by
    the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``
    or :data:`None`.
    Compression is of full batches of data, so the efficacy of batching
    will also impact the compression ratio (more batching means better
    compression). Default: :data:`None`.
    - `max_batch_size`: Maximum size of buffered data per partition.
    After this amount :meth:`send` coroutine will block until batch is
    drained.
    Default: 16384
    - `linger_ms`: The producer groups together any records that arrive
    in between request transmissions into a single batched request.
    Normally this occurs only under load when records arrive faster
    than they can be sent out. However in some circumstances the client
    may want to reduce the number of requests even under moderate load.
    This setting accomplishes this by adding a small amount of
    artificial delay; that is, if first request is processed faster,
    than `linger_ms`, producer will wait ``linger_ms - process_time``.
    Default: 0 (i.e. no delay).
    - `partitioner`: Callable used to determine which partition
    each message is assigned to. Called (after key serialization):
    ``partitioner(key_bytes, all_partitions, available_partitions)``.
    The default partitioner implementation hashes each non-None key
    using the same murmur2 algorithm as the Java client so that
    messages with the same key are assigned to the same partition.
    When a key is :data:`None`, the message is delivered to a random partition
    (filtered to partitions with available leaders only, if possible).
    - `max_request_size`: The maximum size of a request. This is also
    effectively a cap on the maximum record size. Note that the server
    has its own cap on record size which may be different from this.
    This setting will limit the number of record batches the producer
    will send in a single request to avoid sending huge requests.
    Default: 1048576.
    - `metadata_max_age_ms`: The period of time in milliseconds after
    which we force a refresh of metadata even if we haven't seen any
    partition leadership changes to proactively discover any new
    brokers or partitions. Default: 300000
    - `request_timeout_ms`: Produce request timeout in milliseconds.
    As it's sent as part of
    :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking
    call), maximum waiting time can be up to ``2 *
    request_timeout_ms``.
    Default: 40000.
    - `retry_backoff_ms`: Milliseconds to backoff when retrying on
    errors. Default: 100.
    - `api_version`: specify which kafka API version to use.
    If set to ``auto``, will attempt to infer the broker version by
    probing various APIs. Default: ``auto``
    - `security_protocol`: Protocol used to communicate with brokers.
    Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
    Default: ``PLAINTEXT``.
    - `ssl_context`: pre-configured :class:`~ssl.SSLContext`
    for wrapping socket connections. Directly passed into asyncio's
    :meth:`~asyncio.loop.create_connection`. For more
    information see :ref:`ssl_auth`.
    Default: :data:`None`
    - `connections_max_idle_ms`: Close idle connections after the number
    of milliseconds specified by this config. Specifying :data:`None` will
    disable idle checks. Default: 540000 (9 minutes).
    - `enable_idempotence`: When set to :data:`True`, the producer will
    ensure that exactly one copy of each message is written in the
    stream. If :data:`False`, producer retries due to broker failures,
    etc., may write duplicates of the retried message in the stream.
    Note that enabling idempotence acks to set to ``all``. If it is not
    explicitly set by the user it will be chosen. If incompatible
    values are set, a :exc:`ValueError` will be thrown.
    New in version 0.5.0.
    - `sasl_mechanism`: Authentication mechanism when security_protocol
    is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values
    are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
    ``OAUTHBEARER``.
    Default: ``PLAIN``
    - `sasl_plain_username`: username for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_plain_password`: password for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):
    OAuthBearer token provider instance. (See
    :mod:`kafka.oauth.abstract`).
    Default: :data:`None`
    - `*topics`: optional list of topics to subscribe to. If not set,
    call :meth:`.subscribe` or :meth:`.assign` before consuming records.
    Passing topics directly is same as calling :meth:`.subscribe` API.
    - `group_id`: name of the consumer group to join for dynamic
    partition assignment (if enabled), and to use for fetching and
    committing offsets. If None, auto-partition assignment (via
    group coordinator) and offset commits are disabled.
    Default: None
    - `key_deserializer`: Any callable that takes a
    raw message key and returns a deserialized key.
    - `value_deserializer`: Any callable that takes a
    raw message value and returns a deserialized value.
    - `fetch_min_bytes`: Minimum amount of data the server should
    return for a fetch request, otherwise wait up to
    `fetch_max_wait_ms` for more data to accumulate. Default: 1.
    - `fetch_max_bytes`: The maximum amount of data the server should
    return for a fetch request. This is not an absolute maximum, if
    the first message in the first non-empty partition of the fetch
    is larger than this value, the message will still be returned
    to ensure that the consumer can make progress. NOTE: consumer
    performs fetches to multiple brokers in parallel so memory
    usage will depend on the number of brokers containing
    partitions for the topic.
    Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).
    - `fetch_max_wait_ms`: The maximum amount of time in milliseconds
    the server will block before answering the fetch request if
    there isn't sufficient data to immediately satisfy the
    requirement given by fetch_min_bytes. Default: 500.
    - `max_partition_fetch_bytes`: The maximum amount of data
    per-partition the server will return. The maximum total memory
    used for a request ``= #partitions * max_partition_fetch_bytes``.
    This size must be at least as large as the maximum message size
    the server allows or else it is possible for the producer to
    send messages larger than the consumer can fetch. If that
    happens, the consumer can get stuck trying to fetch a large
    message on a certain partition. Default: 1048576.
    - `max_poll_records`: The maximum number of records returned in a
    single call to :meth:`.getmany`. Defaults ``None``, no limit.
    - `auto_offset_reset`: A policy for resetting offsets on
    :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest
    available message, ``latest`` will move to the most recent, and
    ``none`` will raise an exception so you can handle this case.
    Default: ``latest``.
    - `enable_auto_commit`: If true the consumer's offset will be
    periodically committed in the background. Default: True.
    - `auto_commit_interval_ms`: milliseconds between automatic
    offset commits, if enable_auto_commit is True. Default: 5000.
    - `check_crcs`: Automatically check the CRC32 of the records
    consumed. This ensures no on-the-wire or on-disk corruption to
    the messages occurred. This check adds some overhead, so it may
    be disabled in cases seeking extreme performance. Default: True
    - `partition_assignment_strategy`: List of objects to use to
    distribute partition ownership amongst consumer instances when
    group management is used. This preference is implicit in the order
    of the strategies in the list. When assignment strategy changes:
    to support a change to the assignment strategy, new versions must
    enable support both for the old assignment strategy and the new
    one. The coordinator will choose the old assignment strategy until
    all members have been updated. Then it will choose the new
    strategy. Default: [:class:`.RoundRobinPartitionAssignor`]
    - `max_poll_interval_ms`: Maximum allowed time between calls to
    consume messages (e.g., :meth:`.getmany`). If this interval
    is exceeded the consumer is considered failed and the group will
    rebalance in order to reassign the partitions to another consumer
    group member. If API methods block waiting for messages, that time
    does not count against this timeout. See `KIP-62`_ for more
    information. Default 300000
    - `rebalance_timeout_ms`: The maximum time server will wait for this
    consumer to rejoin the group in a case of rebalance. In Java client
    this behaviour is bound to `max.poll.interval.ms` configuration,
    but as ``aiokafka`` will rejoin the group in the background, we
    decouple this setting to allow finer tuning by users that use
    :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults
    to ``session_timeout_ms``
    - `session_timeout_ms`: Client group session and failure detection
    timeout. The consumer sends periodic heartbeats
    (`heartbeat.interval.ms`) to indicate its liveness to the broker.
    If no hearts are received by the broker for a group member within
    the session timeout, the broker will remove the consumer from the
    group and trigger a rebalance. The allowed range is configured with
    the **broker** configuration properties
    `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.
    Default: 10000
    - `heartbeat_interval_ms`: The expected time in milliseconds
    between heartbeats to the consumer coordinator when using
    Kafka's group management feature. Heartbeats are used to ensure
    that the consumer's session stays active and to facilitate
    rebalancing when new consumers join or leave the group. The
    value must be set lower than `session_timeout_ms`, but typically
    should be set no higher than 1/3 of that value. It can be
    adjusted even lower to control the expected time for normal
    rebalances. Default: 3000
    - `consumer_timeout_ms`: maximum wait timeout for background fetching
    routine. Mostly defines how fast the system will see rebalance and
    request new data for new partitions. Default: 200
    - `exclude_internal_topics`: Whether records from internal topics
    (such as offsets) should be exposed to the consumer. If set to True
    the only way to receive records from an internal topic is
    subscribing to it. Requires 0.10+ Default: True
    - `isolation_level`: Controls how to read messages written
    transactionally.

    If set to ``read_committed``, :meth:`.getmany` will only return
    transactional messages which have been committed.
    If set to ``read_uncommitted`` (the default), :meth:`.getmany` will
    return all messages, even transactional messages which have been
    aborted.

    Non-transactional messages will be returned unconditionally in
    either mode.

    Messages will always be returned in offset order. Hence, in
    `read_committed` mode, :meth:`.getmany` will only return
    messages up to the last stable offset (LSO), which is the one less
    than the offset of the first open transaction. In particular any
    messages appearing after messages belonging to ongoing transactions
    will be withheld until the relevant transaction has been completed.
    As a result, `read_committed` consumers will not be able to read up
    to the high watermark when there are in flight transactions.
    Further, when in `read_committed` the seek_to_end method will
    return the LSO. See method docs below. Default: ``read_uncommitted``
    - `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).
    Default: None

    ### `benchmark` {#benchmark}

    `def benchmark(self: fastkafka.FastKafka, interval: Union[int, datetime.timedelta] = 1, sliding_window_size: Optional[int] = None) -> typing.Callable[[typing.Callable[[~I], typing.Optional[~O]]], typing.Callable[[~I], typing.Optional[~O]]]`

    Decorator to benchmark produces/consumes functions

    **Parameters**:
    - `interval`: Period to use to calculate throughput. If value is of type int,
    then it will be used as seconds. If value is of type timedelta,
    then it will be used as it is. default: 1 - one second
    - `sliding_window_size`: The size of the sliding window to use to calculate
    average throughput. default: None - By default average throughput is
    not calculated

    ### `consumes` {#consumes}

    `def consumes(self: fastkafka.FastKafka, topic: Optional[str] = None, decoder: Union[str, Callable[[bytes, pydantic.main.ModelMetaclass], Any]] = 'json', prefix: str = 'on_', loop=None, bootstrap_servers='localhost', client_id='aiokafka-0.8.0', group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, request_timeout_ms=40000, retry_backoff_ms=100, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, metadata_max_age_ms=300000, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, ssl_context=None, security_protocol='PLAINTEXT', api_version='auto', exclude_internal_topics=True, connections_max_idle_ms=540000, isolation_level='read_uncommitted', sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[[pydantic.main.BaseModel], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel], NoneType], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], NoneType]]], typing.Union[typing.Callable[[pydantic.main.BaseModel], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel], NoneType], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], NoneType]]]`

    Decorator registering the callback called when a message is received in a topic.

    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

    **Parameters**:
    - `topic`: Kafka topic that the consumer will subscribe to and execute the
    decorated function when it receives a message from the topic,
    default: None. If the topic is not specified, topic name will be
    inferred from the decorated function name by stripping the defined prefix
    - `decoder`: Decoder to use to decode messages consumed from the topic,
    default: json - By default, it uses json decoder to decode
    bytes to json string and then it creates instance of pydantic
    BaseModel. It also accepts custom decoder function.
    - `prefix`: Prefix stripped from the decorated function to define a topic name
    if the topic argument is not passed, default: "on_". If the decorated
    function name is not prefixed with the defined prefix and topic argument
    is not passed, then this method will throw ValueError
    - `*topics`: optional list of topics to subscribe to. If not set,
    call :meth:`.subscribe` or :meth:`.assign` before consuming records.
    Passing topics directly is same as calling :meth:`.subscribe` API.
    - `bootstrap_servers`: a ``host[:port]`` string (or list of
    ``host[:port]`` strings) that the consumer should contact to bootstrap
    initial cluster metadata.

    This does not have to be the full node list.
    It just needs to have at least one broker that will respond to a
    Metadata API Request. Default port is 9092. If no servers are
    specified, will default to ``localhost:9092``.
    - `client_id`: a name for this client. This string is passed in
    each request to servers and can be used to identify specific
    server-side log entries that correspond to this client. Also
    submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`
    for logging with respect to consumer group administration. Default:
    ``aiokafka-{version}``
    - `group_id`: name of the consumer group to join for dynamic
    partition assignment (if enabled), and to use for fetching and
    committing offsets. If None, auto-partition assignment (via
    group coordinator) and offset commits are disabled.
    Default: None
    - `key_deserializer`: Any callable that takes a
    raw message key and returns a deserialized key.
    - `value_deserializer`: Any callable that takes a
    raw message value and returns a deserialized value.
    - `fetch_min_bytes`: Minimum amount of data the server should
    return for a fetch request, otherwise wait up to
    `fetch_max_wait_ms` for more data to accumulate. Default: 1.
    - `fetch_max_bytes`: The maximum amount of data the server should
    return for a fetch request. This is not an absolute maximum, if
    the first message in the first non-empty partition of the fetch
    is larger than this value, the message will still be returned
    to ensure that the consumer can make progress. NOTE: consumer
    performs fetches to multiple brokers in parallel so memory
    usage will depend on the number of brokers containing
    partitions for the topic.
    Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).
    - `fetch_max_wait_ms`: The maximum amount of time in milliseconds
    the server will block before answering the fetch request if
    there isn't sufficient data to immediately satisfy the
    requirement given by fetch_min_bytes. Default: 500.
    - `max_partition_fetch_bytes`: The maximum amount of data
    per-partition the server will return. The maximum total memory
    used for a request ``= #partitions * max_partition_fetch_bytes``.
    This size must be at least as large as the maximum message size
    the server allows or else it is possible for the producer to
    send messages larger than the consumer can fetch. If that
    happens, the consumer can get stuck trying to fetch a large
    message on a certain partition. Default: 1048576.
    - `max_poll_records`: The maximum number of records returned in a
    single call to :meth:`.getmany`. Defaults ``None``, no limit.
    - `request_timeout_ms`: Client request timeout in milliseconds.
    Default: 40000.
    - `retry_backoff_ms`: Milliseconds to backoff when retrying on
    errors. Default: 100.
    - `auto_offset_reset`: A policy for resetting offsets on
    :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest
    available message, ``latest`` will move to the most recent, and
    ``none`` will raise an exception so you can handle this case.
    Default: ``latest``.
    - `enable_auto_commit`: If true the consumer's offset will be
    periodically committed in the background. Default: True.
    - `auto_commit_interval_ms`: milliseconds between automatic
    offset commits, if enable_auto_commit is True. Default: 5000.
    - `check_crcs`: Automatically check the CRC32 of the records
    consumed. This ensures no on-the-wire or on-disk corruption to
    the messages occurred. This check adds some overhead, so it may
    be disabled in cases seeking extreme performance. Default: True
    - `metadata_max_age_ms`: The period of time in milliseconds after
    which we force a refresh of metadata even if we haven't seen any
    partition leadership changes to proactively discover any new
    brokers or partitions. Default: 300000
    - `partition_assignment_strategy`: List of objects to use to
    distribute partition ownership amongst consumer instances when
    group management is used. This preference is implicit in the order
    of the strategies in the list. When assignment strategy changes:
    to support a change to the assignment strategy, new versions must
    enable support both for the old assignment strategy and the new
    one. The coordinator will choose the old assignment strategy until
    all members have been updated. Then it will choose the new
    strategy. Default: [:class:`.RoundRobinPartitionAssignor`]
    - `max_poll_interval_ms`: Maximum allowed time between calls to
    consume messages (e.g., :meth:`.getmany`). If this interval
    is exceeded the consumer is considered failed and the group will
    rebalance in order to reassign the partitions to another consumer
    group member. If API methods block waiting for messages, that time
    does not count against this timeout. See `KIP-62`_ for more
    information. Default 300000
    - `rebalance_timeout_ms`: The maximum time server will wait for this
    consumer to rejoin the group in a case of rebalance. In Java client
    this behaviour is bound to `max.poll.interval.ms` configuration,
    but as ``aiokafka`` will rejoin the group in the background, we
    decouple this setting to allow finer tuning by users that use
    :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults
    to ``session_timeout_ms``
    - `session_timeout_ms`: Client group session and failure detection
    timeout. The consumer sends periodic heartbeats
    (`heartbeat.interval.ms`) to indicate its liveness to the broker.
    If no hearts are received by the broker for a group member within
    the session timeout, the broker will remove the consumer from the
    group and trigger a rebalance. The allowed range is configured with
    the **broker** configuration properties
    `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.
    Default: 10000
    - `heartbeat_interval_ms`: The expected time in milliseconds
    between heartbeats to the consumer coordinator when using
    Kafka's group management feature. Heartbeats are used to ensure
    that the consumer's session stays active and to facilitate
    rebalancing when new consumers join or leave the group. The
    value must be set lower than `session_timeout_ms`, but typically
    should be set no higher than 1/3 of that value. It can be
    adjusted even lower to control the expected time for normal
    rebalances. Default: 3000
    - `consumer_timeout_ms`: maximum wait timeout for background fetching
    routine. Mostly defines how fast the system will see rebalance and
    request new data for new partitions. Default: 200
    - `api_version`: specify which kafka API version to use.
    :class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.
    If set to ``auto``, will attempt to infer the broker version by
    probing various APIs. Default: ``auto``
    - `security_protocol`: Protocol used to communicate with brokers.
    Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
    - `ssl_context`: pre-configured :class:`~ssl.SSLContext`
    for wrapping socket connections. Directly passed into asyncio's
    :meth:`~asyncio.loop.create_connection`. For more information see
    :ref:`ssl_auth`. Default: None.
    - `exclude_internal_topics`: Whether records from internal topics
    (such as offsets) should be exposed to the consumer. If set to True
    the only way to receive records from an internal topic is
    subscribing to it. Requires 0.10+ Default: True
    - `connections_max_idle_ms`: Close idle connections after the number
    of milliseconds specified by this config. Specifying `None` will
    disable idle checks. Default: 540000 (9 minutes).
    - `isolation_level`: Controls how to read messages written
    transactionally.

    If set to ``read_committed``, :meth:`.getmany` will only return
    transactional messages which have been committed.
    If set to ``read_uncommitted`` (the default), :meth:`.getmany` will
    return all messages, even transactional messages which have been
    aborted.

    Non-transactional messages will be returned unconditionally in
    either mode.

    Messages will always be returned in offset order. Hence, in
    `read_committed` mode, :meth:`.getmany` will only return
    messages up to the last stable offset (LSO), which is the one less
    than the offset of the first open transaction. In particular any
    messages appearing after messages belonging to ongoing transactions
    will be withheld until the relevant transaction has been completed.
    As a result, `read_committed` consumers will not be able to read up
    to the high watermark when there are in flight transactions.
    Further, when in `read_committed` the seek_to_end method will
    return the LSO. See method docs below. Default: ``read_uncommitted``
    - `sasl_mechanism`: Authentication mechanism when security_protocol
    is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:
    ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
    ``OAUTHBEARER``.
    Default: ``PLAIN``
    - `sasl_plain_username`: username for SASL ``PLAIN`` authentication.
    Default: None
    - `sasl_plain_password`: password for SASL ``PLAIN`` authentication.
    Default: None
    - `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).
    Default: None

    **Returns**:
    - : A function returning the same function

    ### `create_mocks` {#create_mocks}

    `def create_mocks(self: fastkafka.FastKafka) -> None`

    Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock

    ### `produces` {#produces}

    `def produces(self: fastkafka.FastKafka, topic: Optional[str] = None, encoder: Union[str, Callable[[pydantic.main.BaseModel], bytes]] = 'json', prefix: str = 'to_', loop=None, bootstrap_servers='localhost', client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]]]]], typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]]]]]`

    Decorator registering the callback called when delivery report for a produced message is received

    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

    **Parameters**:
    - `topic`: Kafka topic that the producer will send returned values from
    the decorated function to, default: None- If the topic is not
    specified, topic name will be inferred from the decorated function
    name by stripping the defined prefix.
    - `encoder`: Encoder to use to encode messages before sending it to topic,
    default: json - By default, it uses json encoder to convert
    pydantic basemodel to json string and then encodes the string to bytes
    using 'utf-8' encoding. It also accepts custom encoder function.
    - `prefix`: Prefix stripped from the decorated function to define a topic
    name if the topic argument is not passed, default: "to_". If the
    decorated function name is not prefixed with the defined prefix
    and topic argument is not passed, then this method will throw ValueError
    - `bootstrap_servers`: a ``host[:port]`` string or list of
    ``host[:port]`` strings that the producer should contact to
    bootstrap initial cluster metadata. This does not have to be the
    full node list.  It just needs to have at least one broker that will
    respond to a Metadata API Request. Default port is 9092. If no
    servers are specified, will default to ``localhost:9092``.
    - `client_id`: a name for this client. This string is passed in
    each request to servers and can be used to identify specific
    server-side log entries that correspond to this client.
    Default: ``aiokafka-producer-#`` (appended with a unique number
    per instance)
    - `key_serializer`: used to convert user-supplied keys to bytes
    If not :data:`None`, called as ``f(key),`` should return
    :class:`bytes`.
    Default: :data:`None`.
    - `value_serializer`: used to convert user-supplied message
    values to :class:`bytes`. If not :data:`None`, called as
    ``f(value)``, should return :class:`bytes`.
    Default: :data:`None`.
    - `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments
    the producer requires the leader to have received before considering a
    request complete. This controls the durability of records that are
    sent. The following settings are common:

    * ``0``: Producer will not wait for any acknowledgment from the server
      at all. The message will immediately be added to the socket
      buffer and considered sent. No guarantee can be made that the
      server has received the record in this case, and the retries
      configuration will not take effect (as the client won't
      generally know of any failures). The offset given back for each
      record will always be set to -1.
    * ``1``: The broker leader will write the record to its local log but
      will respond without awaiting full acknowledgement from all
      followers. In this case should the leader fail immediately
      after acknowledging the record but before the followers have
      replicated it then the record will be lost.
    * ``all``: The broker leader will wait for the full set of in-sync
      replicas to acknowledge the record. This guarantees that the
      record will not be lost as long as at least one in-sync replica
      remains alive. This is the strongest available guarantee.

    If unset, defaults to ``acks=1``. If `enable_idempotence` is
    :data:`True` defaults to ``acks=all``
    - `compression_type`: The compression type for all data generated by
    the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``
    or :data:`None`.
    Compression is of full batches of data, so the efficacy of batching
    will also impact the compression ratio (more batching means better
    compression). Default: :data:`None`.
    - `max_batch_size`: Maximum size of buffered data per partition.
    After this amount :meth:`send` coroutine will block until batch is
    drained.
    Default: 16384
    - `linger_ms`: The producer groups together any records that arrive
    in between request transmissions into a single batched request.
    Normally this occurs only under load when records arrive faster
    than they can be sent out. However in some circumstances the client
    may want to reduce the number of requests even under moderate load.
    This setting accomplishes this by adding a small amount of
    artificial delay; that is, if first request is processed faster,
    than `linger_ms`, producer will wait ``linger_ms - process_time``.
    Default: 0 (i.e. no delay).
    - `partitioner`: Callable used to determine which partition
    each message is assigned to. Called (after key serialization):
    ``partitioner(key_bytes, all_partitions, available_partitions)``.
    The default partitioner implementation hashes each non-None key
    using the same murmur2 algorithm as the Java client so that
    messages with the same key are assigned to the same partition.
    When a key is :data:`None`, the message is delivered to a random partition
    (filtered to partitions with available leaders only, if possible).
    - `max_request_size`: The maximum size of a request. This is also
    effectively a cap on the maximum record size. Note that the server
    has its own cap on record size which may be different from this.
    This setting will limit the number of record batches the producer
    will send in a single request to avoid sending huge requests.
    Default: 1048576.
    - `metadata_max_age_ms`: The period of time in milliseconds after
    which we force a refresh of metadata even if we haven't seen any
    partition leadership changes to proactively discover any new
    brokers or partitions. Default: 300000
    - `request_timeout_ms`: Produce request timeout in milliseconds.
    As it's sent as part of
    :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking
    call), maximum waiting time can be up to ``2 *
    request_timeout_ms``.
    Default: 40000.
    - `retry_backoff_ms`: Milliseconds to backoff when retrying on
    errors. Default: 100.
    - `api_version`: specify which kafka API version to use.
    If set to ``auto``, will attempt to infer the broker version by
    probing various APIs. Default: ``auto``
    - `security_protocol`: Protocol used to communicate with brokers.
    Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
    Default: ``PLAINTEXT``.
    - `ssl_context`: pre-configured :class:`~ssl.SSLContext`
    for wrapping socket connections. Directly passed into asyncio's
    :meth:`~asyncio.loop.create_connection`. For more
    information see :ref:`ssl_auth`.
    Default: :data:`None`
    - `connections_max_idle_ms`: Close idle connections after the number
    of milliseconds specified by this config. Specifying :data:`None` will
    disable idle checks. Default: 540000 (9 minutes).
    - `enable_idempotence`: When set to :data:`True`, the producer will
    ensure that exactly one copy of each message is written in the
    stream. If :data:`False`, producer retries due to broker failures,
    etc., may write duplicates of the retried message in the stream.
    Note that enabling idempotence acks to set to ``all``. If it is not
    explicitly set by the user it will be chosen. If incompatible
    values are set, a :exc:`ValueError` will be thrown.
    New in version 0.5.0.
    - `sasl_mechanism`: Authentication mechanism when security_protocol
    is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values
    are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
    ``OAUTHBEARER``.
    Default: ``PLAIN``
    - `sasl_plain_username`: username for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_plain_password`: password for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):
    OAuthBearer token provider instance. (See
    :mod:`kafka.oauth.abstract`).
    Default: :data:`None`

    **Returns**:
    - : A function returning the same function

    **Exceptions**:
    - `ValueError`: when needed

    ### `run_in_background` {#run_in_background}

    `def run_in_background(self: fastkafka.FastKafka) -> typing.Callable[[typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]], typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]]`

    Decorator to schedule a task to be run in the background.

    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.

    **Returns**:
    - A decorator function that takes a background task as an input and stores it to be run in the backround.


    ## `fastkafka.KafkaEvent` {#fastkafka.KafkaEvent}


    A generic class for representing Kafka events. Based on BaseSubmodel, bound to pydantic.BaseModel

    **Parameters**:
    - `message`: The message contained in the Kafka event, can be of type pydantic.BaseModel.
    - `key`: The optional key used to identify the Kafka event.


    ## `fastkafka.testing.ApacheKafkaBroker` {#fastkafka.testing.ApacheKafkaBroker}


    ApacheKafkaBroker class, used for running unique kafka brokers in tests to prevent topic clashing.

    ### `__init__` {#init}

    `def __init__(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> None`

    Initialises the ApacheKafkaBroker object

    **Parameters**:
    - `data_dir`: Path to the directory where the zookeepeer instance will save data
    - `zookeeper_port`: Port for clients (Kafka brokes) to connect
    - `listener_port`: Port on which the clients (producers and consumers) can connect

    ### `start` {#start}

    `def start(self: fastkafka.testing.ApacheKafkaBroker) -> str`

    Starts a local kafka broker and zookeeper instance synchronously

    **Returns**:
    - Kafka broker bootstrap server address in string format: add:port

    ### `stop` {#stop}

    `def stop(self: fastkafka.testing.ApacheKafkaBroker) -> None`

    Stops a local kafka broker and zookeeper instance synchronously

    **Returns**:
    - None


    ## `fastkafka.testing.LocalRedpandaBroker` {#fastkafka.testing.LocalRedpandaBroker}


    LocalRedpandaBroker class, used for running unique redpanda brokers in tests to prevent topic clashing.

    ### `__init__` {#init}

    `def __init__(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, listener_port: int = 9092, tag: str = 'v23.1.2', seastar_core: int = 1, memory: str = '1G', mode: str = 'dev-container', default_log_level: str = 'debug', **kwargs: Dict[str, Any]) -> None`

    Initialises the LocalRedpandaBroker object

    **Parameters**:
    - `listener_port`: Port on which the clients (producers and consumers) can connect
    - `tag`: Tag of Redpanda image to use to start container
    - `seastar_core`: Core(s) to use byt Seastar (the framework Redpanda uses under the hood)
    - `memory`: The amount of memory to make available to Redpanda
    - `mode`: Mode to use to load configuration properties in container
    - `default_log_level`: Log levels to use for Redpanda

    ### `get_service_config_string` {#get_service_config_string}

    `def get_service_config_string(self, service: str, data_dir: pathlib.Path) -> str`

    Generates a configuration for a service

    **Parameters**:
    - `data_dir`: Path to the directory where the zookeepeer instance will save data
    - `service`: "redpanda", defines which service to get config string for

    ### `start` {#start}

    `def start(self: fastkafka.testing.LocalRedpandaBroker) -> str`

    Starts a local redpanda broker instance synchronously

    **Returns**:
    - Redpanda broker bootstrap server address in string format: add:port

    ### `stop` {#stop}

    `def stop(self: fastkafka.testing.LocalRedpandaBroker) -> None`

    Stops a local redpanda broker instance synchronously

    **Returns**:
    - None


    ## `fastkafka.testing.Tester` {#fastkafka.testing.Tester}

    ### `__init__` {#init}

    `def __init__(self, app: Union[fastkafka.FastKafka, List[fastkafka.FastKafka]], broker: Optional[fastkafka.testing.ApacheKafkaBroker, fastkafka.testing.LocalRedpandaBroker, fastkafka._testing.in_memory_broker.InMemoryBroker] = None, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> None`

    Mirror-like object for testing a FastFafka application

    Can be used as context manager

    **Parameters**:
    - `data_dir`: Path to the directory where the zookeepeer instance will save data
    - `zookeeper_port`: Port for clients (Kafka brokes) to connect
    - `listener_port`: Port on which the clients (producers and consumers) can connect

    ### `benchmark` {#benchmark}

    `def benchmark(self: fastkafka.FastKafka, interval: Union[int, datetime.timedelta] = 1, sliding_window_size: Optional[int] = None) -> typing.Callable[[typing.Callable[[~I], typing.Optional[~O]]], typing.Callable[[~I], typing.Optional[~O]]]`

    Decorator to benchmark produces/consumes functions

    **Parameters**:
    - `interval`: Period to use to calculate throughput. If value is of type int,
    then it will be used as seconds. If value is of type timedelta,
    then it will be used as it is. default: 1 - one second
    - `sliding_window_size`: The size of the sliding window to use to calculate
    average throughput. default: None - By default average throughput is
    not calculated

    ### `consumes` {#consumes}

    `def consumes(self: fastkafka.FastKafka, topic: Optional[str] = None, decoder: Union[str, Callable[[bytes, pydantic.main.ModelMetaclass], Any]] = 'json', prefix: str = 'on_', loop=None, bootstrap_servers='localhost', client_id='aiokafka-0.8.0', group_id=None, key_deserializer=None, value_deserializer=None, fetch_max_wait_ms=500, fetch_max_bytes=52428800, fetch_min_bytes=1, max_partition_fetch_bytes=1048576, request_timeout_ms=40000, retry_backoff_ms=100, auto_offset_reset='latest', enable_auto_commit=True, auto_commit_interval_ms=5000, check_crcs=True, metadata_max_age_ms=300000, partition_assignment_strategy=(<class 'kafka.coordinator.assignors.roundrobin.RoundRobinPartitionAssignor'>,), max_poll_interval_ms=300000, rebalance_timeout_ms=None, session_timeout_ms=10000, heartbeat_interval_ms=3000, consumer_timeout_ms=200, max_poll_records=None, ssl_context=None, security_protocol='PLAINTEXT', api_version='auto', exclude_internal_topics=True, connections_max_idle_ms=540000, isolation_level='read_uncommitted', sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[[pydantic.main.BaseModel], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel], NoneType], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], NoneType]]], typing.Union[typing.Callable[[pydantic.main.BaseModel], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], typing.Awaitable[NoneType]], typing.Callable[[pydantic.main.BaseModel], NoneType], typing.Callable[[pydantic.main.BaseModel, fastkafka.EventMetadata], NoneType]]]`

    Decorator registering the callback called when a message is received in a topic.

    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

    **Parameters**:
    - `topic`: Kafka topic that the consumer will subscribe to and execute the
    decorated function when it receives a message from the topic,
    default: None. If the topic is not specified, topic name will be
    inferred from the decorated function name by stripping the defined prefix
    - `decoder`: Decoder to use to decode messages consumed from the topic,
    default: json - By default, it uses json decoder to decode
    bytes to json string and then it creates instance of pydantic
    BaseModel. It also accepts custom decoder function.
    - `prefix`: Prefix stripped from the decorated function to define a topic name
    if the topic argument is not passed, default: "on_". If the decorated
    function name is not prefixed with the defined prefix and topic argument
    is not passed, then this method will throw ValueError
    - `*topics`: optional list of topics to subscribe to. If not set,
    call :meth:`.subscribe` or :meth:`.assign` before consuming records.
    Passing topics directly is same as calling :meth:`.subscribe` API.
    - `bootstrap_servers`: a ``host[:port]`` string (or list of
    ``host[:port]`` strings) that the consumer should contact to bootstrap
    initial cluster metadata.

    This does not have to be the full node list.
    It just needs to have at least one broker that will respond to a
    Metadata API Request. Default port is 9092. If no servers are
    specified, will default to ``localhost:9092``.
    - `client_id`: a name for this client. This string is passed in
    each request to servers and can be used to identify specific
    server-side log entries that correspond to this client. Also
    submitted to :class:`~.consumer.group_coordinator.GroupCoordinator`
    for logging with respect to consumer group administration. Default:
    ``aiokafka-{version}``
    - `group_id`: name of the consumer group to join for dynamic
    partition assignment (if enabled), and to use for fetching and
    committing offsets. If None, auto-partition assignment (via
    group coordinator) and offset commits are disabled.
    Default: None
    - `key_deserializer`: Any callable that takes a
    raw message key and returns a deserialized key.
    - `value_deserializer`: Any callable that takes a
    raw message value and returns a deserialized value.
    - `fetch_min_bytes`: Minimum amount of data the server should
    return for a fetch request, otherwise wait up to
    `fetch_max_wait_ms` for more data to accumulate. Default: 1.
    - `fetch_max_bytes`: The maximum amount of data the server should
    return for a fetch request. This is not an absolute maximum, if
    the first message in the first non-empty partition of the fetch
    is larger than this value, the message will still be returned
    to ensure that the consumer can make progress. NOTE: consumer
    performs fetches to multiple brokers in parallel so memory
    usage will depend on the number of brokers containing
    partitions for the topic.
    Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).
    - `fetch_max_wait_ms`: The maximum amount of time in milliseconds
    the server will block before answering the fetch request if
    there isn't sufficient data to immediately satisfy the
    requirement given by fetch_min_bytes. Default: 500.
    - `max_partition_fetch_bytes`: The maximum amount of data
    per-partition the server will return. The maximum total memory
    used for a request ``= #partitions * max_partition_fetch_bytes``.
    This size must be at least as large as the maximum message size
    the server allows or else it is possible for the producer to
    send messages larger than the consumer can fetch. If that
    happens, the consumer can get stuck trying to fetch a large
    message on a certain partition. Default: 1048576.
    - `max_poll_records`: The maximum number of records returned in a
    single call to :meth:`.getmany`. Defaults ``None``, no limit.
    - `request_timeout_ms`: Client request timeout in milliseconds.
    Default: 40000.
    - `retry_backoff_ms`: Milliseconds to backoff when retrying on
    errors. Default: 100.
    - `auto_offset_reset`: A policy for resetting offsets on
    :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest
    available message, ``latest`` will move to the most recent, and
    ``none`` will raise an exception so you can handle this case.
    Default: ``latest``.
    - `enable_auto_commit`: If true the consumer's offset will be
    periodically committed in the background. Default: True.
    - `auto_commit_interval_ms`: milliseconds between automatic
    offset commits, if enable_auto_commit is True. Default: 5000.
    - `check_crcs`: Automatically check the CRC32 of the records
    consumed. This ensures no on-the-wire or on-disk corruption to
    the messages occurred. This check adds some overhead, so it may
    be disabled in cases seeking extreme performance. Default: True
    - `metadata_max_age_ms`: The period of time in milliseconds after
    which we force a refresh of metadata even if we haven't seen any
    partition leadership changes to proactively discover any new
    brokers or partitions. Default: 300000
    - `partition_assignment_strategy`: List of objects to use to
    distribute partition ownership amongst consumer instances when
    group management is used. This preference is implicit in the order
    of the strategies in the list. When assignment strategy changes:
    to support a change to the assignment strategy, new versions must
    enable support both for the old assignment strategy and the new
    one. The coordinator will choose the old assignment strategy until
    all members have been updated. Then it will choose the new
    strategy. Default: [:class:`.RoundRobinPartitionAssignor`]
    - `max_poll_interval_ms`: Maximum allowed time between calls to
    consume messages (e.g., :meth:`.getmany`). If this interval
    is exceeded the consumer is considered failed and the group will
    rebalance in order to reassign the partitions to another consumer
    group member. If API methods block waiting for messages, that time
    does not count against this timeout. See `KIP-62`_ for more
    information. Default 300000
    - `rebalance_timeout_ms`: The maximum time server will wait for this
    consumer to rejoin the group in a case of rebalance. In Java client
    this behaviour is bound to `max.poll.interval.ms` configuration,
    but as ``aiokafka`` will rejoin the group in the background, we
    decouple this setting to allow finer tuning by users that use
    :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults
    to ``session_timeout_ms``
    - `session_timeout_ms`: Client group session and failure detection
    timeout. The consumer sends periodic heartbeats
    (`heartbeat.interval.ms`) to indicate its liveness to the broker.
    If no hearts are received by the broker for a group member within
    the session timeout, the broker will remove the consumer from the
    group and trigger a rebalance. The allowed range is configured with
    the **broker** configuration properties
    `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.
    Default: 10000
    - `heartbeat_interval_ms`: The expected time in milliseconds
    between heartbeats to the consumer coordinator when using
    Kafka's group management feature. Heartbeats are used to ensure
    that the consumer's session stays active and to facilitate
    rebalancing when new consumers join or leave the group. The
    value must be set lower than `session_timeout_ms`, but typically
    should be set no higher than 1/3 of that value. It can be
    adjusted even lower to control the expected time for normal
    rebalances. Default: 3000
    - `consumer_timeout_ms`: maximum wait timeout for background fetching
    routine. Mostly defines how fast the system will see rebalance and
    request new data for new partitions. Default: 200
    - `api_version`: specify which kafka API version to use.
    :class:`AIOKafkaConsumer` supports Kafka API versions >=0.9 only.
    If set to ``auto``, will attempt to infer the broker version by
    probing various APIs. Default: ``auto``
    - `security_protocol`: Protocol used to communicate with brokers.
    Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
    - `ssl_context`: pre-configured :class:`~ssl.SSLContext`
    for wrapping socket connections. Directly passed into asyncio's
    :meth:`~asyncio.loop.create_connection`. For more information see
    :ref:`ssl_auth`. Default: None.
    - `exclude_internal_topics`: Whether records from internal topics
    (such as offsets) should be exposed to the consumer. If set to True
    the only way to receive records from an internal topic is
    subscribing to it. Requires 0.10+ Default: True
    - `connections_max_idle_ms`: Close idle connections after the number
    of milliseconds specified by this config. Specifying `None` will
    disable idle checks. Default: 540000 (9 minutes).
    - `isolation_level`: Controls how to read messages written
    transactionally.

    If set to ``read_committed``, :meth:`.getmany` will only return
    transactional messages which have been committed.
    If set to ``read_uncommitted`` (the default), :meth:`.getmany` will
    return all messages, even transactional messages which have been
    aborted.

    Non-transactional messages will be returned unconditionally in
    either mode.

    Messages will always be returned in offset order. Hence, in
    `read_committed` mode, :meth:`.getmany` will only return
    messages up to the last stable offset (LSO), which is the one less
    than the offset of the first open transaction. In particular any
    messages appearing after messages belonging to ongoing transactions
    will be withheld until the relevant transaction has been completed.
    As a result, `read_committed` consumers will not be able to read up
    to the high watermark when there are in flight transactions.
    Further, when in `read_committed` the seek_to_end method will
    return the LSO. See method docs below. Default: ``read_uncommitted``
    - `sasl_mechanism`: Authentication mechanism when security_protocol
    is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values are:
    ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
    ``OAUTHBEARER``.
    Default: ``PLAIN``
    - `sasl_plain_username`: username for SASL ``PLAIN`` authentication.
    Default: None
    - `sasl_plain_password`: password for SASL ``PLAIN`` authentication.
    Default: None
    - `sasl_oauth_token_provider`: OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).
    Default: None

    **Returns**:
    - : A function returning the same function

    ### `create_mocks` {#create_mocks}

    `def create_mocks(self: fastkafka.FastKafka) -> None`

    Creates self.mocks as a named tuple mapping a new function obtained by calling the original functions and a mock

    ### `produces` {#produces}

    `def produces(self: fastkafka.FastKafka, topic: Optional[str] = None, encoder: Union[str, Callable[[pydantic.main.BaseModel], bytes]] = 'json', prefix: str = 'to_', loop=None, bootstrap_servers='localhost', client_id=None, metadata_max_age_ms=300000, request_timeout_ms=40000, api_version='auto', acks=<object object>, key_serializer=None, value_serializer=None, compression_type=None, max_batch_size=16384, partitioner=<kafka.partitioner.default.DefaultPartitioner object>, max_request_size=1048576, linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100, security_protocol='PLAINTEXT', ssl_context=None, connections_max_idle_ms=540000, enable_idempotence=False, transactional_id=None, transaction_timeout_ms=60000, sasl_mechanism='PLAIN', sasl_plain_password=None, sasl_plain_username=None, sasl_kerberos_service_name='kafka', sasl_kerberos_domain_name=None, sasl_oauth_token_provider=None) -> typing.Callable[[typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]]]]], typing.Union[typing.Callable[..., typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]], typing.Callable[..., typing.Awaitable[typing.Union[pydantic.main.BaseModel, fastkafka.KafkaEvent[pydantic.main.BaseModel], typing.List[pydantic.main.BaseModel], fastkafka.KafkaEvent[typing.List[pydantic.main.BaseModel]]]]]]]`

    Decorator registering the callback called when delivery report for a produced message is received

    This function decorator is also responsible for registering topics for AsyncAPI specificiation and documentation.

    **Parameters**:
    - `topic`: Kafka topic that the producer will send returned values from
    the decorated function to, default: None- If the topic is not
    specified, topic name will be inferred from the decorated function
    name by stripping the defined prefix.
    - `encoder`: Encoder to use to encode messages before sending it to topic,
    default: json - By default, it uses json encoder to convert
    pydantic basemodel to json string and then encodes the string to bytes
    using 'utf-8' encoding. It also accepts custom encoder function.
    - `prefix`: Prefix stripped from the decorated function to define a topic
    name if the topic argument is not passed, default: "to_". If the
    decorated function name is not prefixed with the defined prefix
    and topic argument is not passed, then this method will throw ValueError
    - `bootstrap_servers`: a ``host[:port]`` string or list of
    ``host[:port]`` strings that the producer should contact to
    bootstrap initial cluster metadata. This does not have to be the
    full node list.  It just needs to have at least one broker that will
    respond to a Metadata API Request. Default port is 9092. If no
    servers are specified, will default to ``localhost:9092``.
    - `client_id`: a name for this client. This string is passed in
    each request to servers and can be used to identify specific
    server-side log entries that correspond to this client.
    Default: ``aiokafka-producer-#`` (appended with a unique number
    per instance)
    - `key_serializer`: used to convert user-supplied keys to bytes
    If not :data:`None`, called as ``f(key),`` should return
    :class:`bytes`.
    Default: :data:`None`.
    - `value_serializer`: used to convert user-supplied message
    values to :class:`bytes`. If not :data:`None`, called as
    ``f(value)``, should return :class:`bytes`.
    Default: :data:`None`.
    - `acks`: one of ``0``, ``1``, ``all``. The number of acknowledgments
    the producer requires the leader to have received before considering a
    request complete. This controls the durability of records that are
    sent. The following settings are common:

    * ``0``: Producer will not wait for any acknowledgment from the server
      at all. The message will immediately be added to the socket
      buffer and considered sent. No guarantee can be made that the
      server has received the record in this case, and the retries
      configuration will not take effect (as the client won't
      generally know of any failures). The offset given back for each
      record will always be set to -1.
    * ``1``: The broker leader will write the record to its local log but
      will respond without awaiting full acknowledgement from all
      followers. In this case should the leader fail immediately
      after acknowledging the record but before the followers have
      replicated it then the record will be lost.
    * ``all``: The broker leader will wait for the full set of in-sync
      replicas to acknowledge the record. This guarantees that the
      record will not be lost as long as at least one in-sync replica
      remains alive. This is the strongest available guarantee.

    If unset, defaults to ``acks=1``. If `enable_idempotence` is
    :data:`True` defaults to ``acks=all``
    - `compression_type`: The compression type for all data generated by
    the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``
    or :data:`None`.
    Compression is of full batches of data, so the efficacy of batching
    will also impact the compression ratio (more batching means better
    compression). Default: :data:`None`.
    - `max_batch_size`: Maximum size of buffered data per partition.
    After this amount :meth:`send` coroutine will block until batch is
    drained.
    Default: 16384
    - `linger_ms`: The producer groups together any records that arrive
    in between request transmissions into a single batched request.
    Normally this occurs only under load when records arrive faster
    than they can be sent out. However in some circumstances the client
    may want to reduce the number of requests even under moderate load.
    This setting accomplishes this by adding a small amount of
    artificial delay; that is, if first request is processed faster,
    than `linger_ms`, producer will wait ``linger_ms - process_time``.
    Default: 0 (i.e. no delay).
    - `partitioner`: Callable used to determine which partition
    each message is assigned to. Called (after key serialization):
    ``partitioner(key_bytes, all_partitions, available_partitions)``.
    The default partitioner implementation hashes each non-None key
    using the same murmur2 algorithm as the Java client so that
    messages with the same key are assigned to the same partition.
    When a key is :data:`None`, the message is delivered to a random partition
    (filtered to partitions with available leaders only, if possible).
    - `max_request_size`: The maximum size of a request. This is also
    effectively a cap on the maximum record size. Note that the server
    has its own cap on record size which may be different from this.
    This setting will limit the number of record batches the producer
    will send in a single request to avoid sending huge requests.
    Default: 1048576.
    - `metadata_max_age_ms`: The period of time in milliseconds after
    which we force a refresh of metadata even if we haven't seen any
    partition leadership changes to proactively discover any new
    brokers or partitions. Default: 300000
    - `request_timeout_ms`: Produce request timeout in milliseconds.
    As it's sent as part of
    :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking
    call), maximum waiting time can be up to ``2 *
    request_timeout_ms``.
    Default: 40000.
    - `retry_backoff_ms`: Milliseconds to backoff when retrying on
    errors. Default: 100.
    - `api_version`: specify which kafka API version to use.
    If set to ``auto``, will attempt to infer the broker version by
    probing various APIs. Default: ``auto``
    - `security_protocol`: Protocol used to communicate with brokers.
    Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
    Default: ``PLAINTEXT``.
    - `ssl_context`: pre-configured :class:`~ssl.SSLContext`
    for wrapping socket connections. Directly passed into asyncio's
    :meth:`~asyncio.loop.create_connection`. For more
    information see :ref:`ssl_auth`.
    Default: :data:`None`
    - `connections_max_idle_ms`: Close idle connections after the number
    of milliseconds specified by this config. Specifying :data:`None` will
    disable idle checks. Default: 540000 (9 minutes).
    - `enable_idempotence`: When set to :data:`True`, the producer will
    ensure that exactly one copy of each message is written in the
    stream. If :data:`False`, producer retries due to broker failures,
    etc., may write duplicates of the retried message in the stream.
    Note that enabling idempotence acks to set to ``all``. If it is not
    explicitly set by the user it will be chosen. If incompatible
    values are set, a :exc:`ValueError` will be thrown.
    New in version 0.5.0.
    - `sasl_mechanism`: Authentication mechanism when security_protocol
    is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values
    are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
    ``OAUTHBEARER``.
    Default: ``PLAIN``
    - `sasl_plain_username`: username for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_plain_password`: password for SASL ``PLAIN`` authentication.
    Default: :data:`None`
    - `sasl_oauth_token_provider (`: class:`~aiokafka.abc.AbstractTokenProvider`):
    OAuthBearer token provider instance. (See
    :mod:`kafka.oauth.abstract`).
    Default: :data:`None`

    **Returns**:
    - : A function returning the same function

    **Exceptions**:
    - `ValueError`: when needed

    ### `run_in_background` {#run_in_background}

    `def run_in_background(self: fastkafka.FastKafka) -> typing.Callable[[typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]], typing.Callable[..., typing.Coroutine[typing.Any, typing.Any, typing.Any]]]`

    Decorator to schedule a task to be run in the background.

    This decorator is used to schedule a task to be run in the background when the app's `_on_startup` event is triggered.

    **Returns**:
    - A decorator function that takes a background task as an input and stores it to be run in the backround.

    ### `using_local_kafka` {#using_local_kafka}

    `def using_local_kafka(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, zookeeper_port: int = 2181, listener_port: int = 9092) -> Tester`

    Starts local Kafka broker used by the Tester instance

    **Parameters**:
    - `data_dir`: Path to the directory where the zookeepeer instance will save data
    - `zookeeper_port`: Port for clients (Kafka brokes) to connect
    - `listener_port`: Port on which the clients (producers and consumers) can connect

    **Returns**:
    - An instance of tester with Kafka as broker

    ### `using_local_redpanda` {#using_local_redpanda}

    `def using_local_redpanda(self, topics: Iterable[str] = [], retries: int = 3, apply_nest_asyncio: bool = False, listener_port: int = 9092, tag: str = 'v23.1.2', seastar_core: int = 1, memory: str = '1G', mode: str = 'dev-container', default_log_level: str = 'debug') -> Tester`

    Starts local Redpanda broker used by the Tester instance

    **Parameters**:
    - `listener_port`: Port on which the clients (producers and consumers) can connect
    - `tag`: Tag of Redpanda image to use to start container
    - `seastar_core`: Core(s) to use byt Seastar (the framework Redpanda uses under the hood)
    - `memory`: The amount of memory to make available to Redpanda
    - `mode`: Mode to use to load configuration properties in container
    - `default_log_level`: Log levels to use for Redpanda

    **Returns**:
    - An instance of tester with Redpanda as broker

``` python
fixture = ['            - [json_encoder](api/fastkafka/encoder/json_encoder.md)', '        - testing', '            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)', '            - [LocalRedpandaBroker](api/fastkafka/testing/LocalRedpandaBroker.md)', '            - [Tester](api/fastkafka/testing/Tester.md)']
expected = (['api/fastkafka/encoder/json_encoder'], 1)

actual = _parse_lines(fixture)
print(actual)

assert actual == expected, actual
```

    (['api/fastkafka/encoder/json_encoder'], 1)

``` python
fixture = ['            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)', '            - [LocalRedpandaBroker](api/fastkafka/testing/LocalRedpandaBroker.md)', '            - [Tester](api/fastkafka/testing/Tester.md)']
expected = (['api/fastkafka/testing/ApacheKafkaBroker', 'api/fastkafka/testing/LocalRedpandaBroker', 'api/fastkafka/testing/Tester'], 3)

actual = _parse_lines(fixture)
print(actual)

assert actual == expected, actual
```

    (['api/fastkafka/testing/ApacheKafkaBroker', 'api/fastkafka/testing/LocalRedpandaBroker', 'api/fastkafka/testing/Tester'], 3)

``` python
fixture = """    - fastkafka
        - [FastKafka](api/fastkafka/FastKafka.md)
        - [KafkaEvent](api/fastkafka/KafkaEvent.md)
        - encoder
            - [json_encoder](api/fastkafka/encoder/json_encoder.md)
        - testing
            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)
            - [LocalRedpandaBroker](api/fastkafka/testing/LocalRedpandaBroker.md)
            - [Tester](api/fastkafka/testing/Tester.md)
"""

expected = [
    "api/fastkafka/FastKafka",
    "api/fastkafka/KafkaEvent",
    {"encoder": ["api/fastkafka/encoder/json_encoder"]},
    {
        "testing": [
            "api/fastkafka/testing/ApacheKafkaBroker",
            "api/fastkafka/testing/LocalRedpandaBroker",
            "api/fastkafka/testing/Tester",
        ]
    }
]

ignore_first_line = True
actual = _parse_section(fixture, ignore_first_line)
display(actual)
assert actual == expected
```

    ['api/fastkafka/FastKafka',
     'api/fastkafka/KafkaEvent',
     {'encoder': ['api/fastkafka/encoder/json_encoder']},
     {'testing': ['api/fastkafka/testing/ApacheKafkaBroker',
       'api/fastkafka/testing/LocalRedpandaBroker',
       'api/fastkafka/testing/Tester']}]

``` python
fixture = """    - Writing services
        - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
        - [@consumes basics](guides/Guide_21_Produces_Basics.md)
    - Testing
        - [Using Redpanda to test FastKafka](guides/Guide_31_Using_redpanda_to_test_fastkafka.md)
"""

expected = [
    {
        "Writing services": [
            "guides/Guide_11_Consumes_Basics",
            "guides/Guide_21_Produces_Basics",
        ],
    },
    {
        "Testing": ["guides/Guide_31_Using_redpanda_to_test_fastkafka"],
    },
]

actual = _parse_section(fixture)
display(actual)
assert actual == expected
```

    [{'Writing services': ['guides/Guide_11_Consumes_Basics',
       'guides/Guide_21_Produces_Basics']},
     {'Testing': ['guides/Guide_31_Using_redpanda_to_test_fastkafka']}]

``` python
summary = """- [FastKafka](index.md)
- Guides
    - Writing services
        - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
        - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
    - Testing
        - [Using Redpanda to test FastKafka](guides/Guide_31_Using_redpanda_to_test_fastkafka.md)
- API
    - fastkafka
        - [FastKafka](api/fastkafka/FastKafka.md)
        - [KafkaEvent](api/fastkafka/KafkaEvent.md)
        - encoder
            - [json_encoder](api/fastkafka/encoder/json_encoder.md)
        - testing
            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)
            - [LocalRedpandaBroker](api/fastkafka/testing/LocalRedpandaBroker.md)
            - [Tester](api/fastkafka/testing/Tester.md)
- CLI
    - [fastkafka](cli/fastkafka.md)
    - [run_fastkafka_server_process](cli/run_fastkafka_server_process.md)
- [Releases](CHANGELOG.md)"""

section_header = "API"
expected = """    - fastkafka
        - [FastKafka](api/fastkafka/FastKafka.md)
        - [KafkaEvent](api/fastkafka/KafkaEvent.md)
        - encoder
            - [json_encoder](api/fastkafka/encoder/json_encoder.md)
        - testing
            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)
            - [LocalRedpandaBroker](api/fastkafka/testing/LocalRedpandaBroker.md)
            - [Tester](api/fastkafka/testing/Tester.md)
"""
actual = _get_section_from_markdown(summary, section_header)
# print(actual)
assert actual == expected
```

``` python
section_header = "CLI"
expected = """    - [fastkafka](cli/fastkafka.md)
    - [run_fastkafka_server_process](cli/run_fastkafka_server_process.md)
"""
actual = _get_section_from_markdown(summary, section_header)
print(actual)
assert actual == expected
```

        - [fastkafka](cli/fastkafka.md)
        - [run_fastkafka_server_process](cli/run_fastkafka_server_process.md)

``` python
section_header = "Guides"
expected = """    - Writing services
        - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
        - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
    - Testing
        - [Using Redpanda to test FastKafka](guides/Guide_31_Using_redpanda_to_test_fastkafka.md)
"""
actual = _get_section_from_markdown(summary, section_header)
print(actual)
assert actual == expected
```

        - Writing services
            - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
            - [@consumes basics](guides/Guide_11_Consumes_Basics.md)
        - Testing
            - [Using Redpanda to test FastKafka](guides/Guide_31_Using_redpanda_to_test_fastkafka.md)

``` python
section_header = "Invalid Section"
expected = None
actual = _get_section_from_markdown(summary, section_header)
print(actual)
assert actual == expected
```

    None

------------------------------------------------------------------------

### generate_sidebar

>      generate_sidebar (summary_file:str='./docusaurus/docs/SUMMARY.md',
>                        summary:str='', target:str='./docusaurus/sidebars.js')

``` python
summary = """- [FastKafka](index.md)
- Guides
    - Writing services
        - [Lifespan Events](guides/Guide_05_Lifespan_Handler.md)
        - [Encoding and Decoding Kafka Messages with FastKafka](guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka.md)
    - Testing
        - [Using Redpanda to test FastKafka](guides/Guide_31_Using_redpanda_to_test_fastkafka.md)
    - Documentation generation
        - [Deploy FastKafka docs to GitHub Pages](guides/Guide_04_Github_Actions_Workflow.md)
- API
    - fastkafka
        - [FastKafka](api/fastkafka/FastKafka.md)
        - [KafkaEvent](api/fastkafka/KafkaEvent.md)
        - encoder
            - [AvroBase](api/fastkafka/encoder/AvroBase.md)
            - [json_decoder](api/fastkafka/encoder/json_decoder.md)
            - [json_encoder](api/fastkafka/encoder/json_encoder.md)
        - testing
            - [ApacheKafkaBroker](api/fastkafka/testing/ApacheKafkaBroker.md)
- CLI
    - [fastkafka](cli/fastkafka.md)
    - [run_fastkafka_server_process](cli/run_fastkafka_server_process.md)
- [Releases](CHANGELOG.md)"""

with TemporaryDirectory() as directory:
    with open(directory + "/SUMMARY.md", "w") as stream:
        stream.write(summary)

    generate_sidebar(
        summary_file=directory + "/SUMMARY.md", target=directory + "/test.js"
    )

    with open(directory + "/test.js") as stream:
        stream = stream.read()

print(stream)
assert (
    stream
    == """module.exports = {
tutorialSidebar: [
    'index', {'Guides': 
    [{'Writing services': ['guides/Guide_05_Lifespan_Handler', 'guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka']}, {'Testing': ['guides/Guide_31_Using_redpanda_to_test_fastkafka']}, {'Documentation generation': ['guides/Guide_04_Github_Actions_Workflow']}]},{'API': ['api/fastkafka/FastKafka', 'api/fastkafka/KafkaEvent', {'encoder': ['api/fastkafka/encoder/AvroBase', 'api/fastkafka/encoder/json_decoder', 'api/fastkafka/encoder/json_encoder']}, {'testing': ['api/fastkafka/testing/ApacheKafkaBroker']}]},{'CLI': ['cli/fastkafka', 'cli/run_fastkafka_server_process']},
    "LICENSE",
    "CONTRIBUTING",
    "CHANGELOG",
],
};"""
), stream
```

    module.exports = {
    tutorialSidebar: [
        'index', {'Guides': 
        [{'Writing services': ['guides/Guide_05_Lifespan_Handler', 'guides/Guide_07_Encoding_and_Decoding_Messages_with_FastKafka']}, {'Testing': ['guides/Guide_31_Using_redpanda_to_test_fastkafka']}, {'Documentation generation': ['guides/Guide_04_Github_Actions_Workflow']}]},{'API': ['api/fastkafka/FastKafka', 'api/fastkafka/KafkaEvent', {'encoder': ['api/fastkafka/encoder/AvroBase', 'api/fastkafka/encoder/json_decoder', 'api/fastkafka/encoder/json_encoder']}, {'testing': ['api/fastkafka/testing/ApacheKafkaBroker']}]},{'CLI': ['cli/fastkafka', 'cli/run_fastkafka_server_process']},
        "LICENSE",
        "CONTRIBUTING",
        "CHANGELOG",
    ],
    };
