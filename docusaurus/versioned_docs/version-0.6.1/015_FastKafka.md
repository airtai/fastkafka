
<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

``` python
import asyncer

from fastkafka._components.logger import supress_timestamps
from fastkafka.encoder import avro_decoder, avro_encoder, json_decoder, json_encoder
from fastkafka.testing import Tester
```

``` python
supress_timestamps()
logger = get_logger(__name__, level=20)
logger.info("ok")
```

    [INFO] __main__: ok

``` python
import os
import shutil
import unittest.mock
from contextlib import asynccontextmanager

import pytest
import yaml
from pydantic import EmailStr, Field, HttpUrl

from fastkafka._components.helpers import true_after
from fastkafka.testing import ApacheKafkaBroker, mock_AIOKafkaProducer_send
```

``` python
# allows async calls in notebooks

import nest_asyncio
```

``` python
nest_asyncio.apply()
```

### Constructor utilities

``` python
assert _get_kafka_config() == {
    "bootstrap_servers": "localhost:9092",
    "auto_offset_reset": "earliest",
    "max_poll_records": 100,
}

assert _get_kafka_config(max_poll_records=1_000) == {
    "bootstrap_servers": "localhost:9092",
    "auto_offset_reset": "earliest",
    "max_poll_records": 1_000,
}
```

``` python
with pytest.raises(ValueError) as e:
    _get_kafka_config(random_key=1_000, whatever="whocares")
assert e.value.args == ("Unallowed key arguments passed: 'random_key', 'whatever'",)
```

``` python
assert (
    _get_kafka_brokers(None).json()
    == '{"brokers": {"localhost": {"url": "https://localhost", "description": "Local (dev) Kafka broker", "protocol": "kafka", "variables": {"port": {"default": "9092"}}}}}'
)

assert (
    _get_kafka_brokers(dict(localhost=dict(url="localhost"))).json()
    == '{"brokers": {"localhost": {"url": "localhost", "description": "Kafka broker", "protocol": "kafka", "variables": {"port": {"default": "9092"}}}}}'
)

assert (
    _get_kafka_brokers(
        dict(localhost=dict(url="localhost"), staging=dict(url="staging.airt.ai"))
    ).json()
    == '{"brokers": {"localhost": {"url": "localhost", "description": "Kafka broker", "protocol": "kafka", "variables": {"port": {"default": "9092"}}}, "staging": {"url": "staging.airt.ai", "description": "Kafka broker", "protocol": "kafka", "variables": {"port": {"default": "9092"}}}}}'
)
```

``` python
def on_topic_name_1():
    pass


assert _get_topic_name(on_topic_name_1) == "topic_name_1"

assert _get_topic_name(on_topic_name_1, prefix="on_topic_") == "name_1"
```

``` python
assert _get_contact_info() == ContactInfo(
    name="Author",
    url=HttpUrl(url="https://www.google.com", scheme="http"),
    email="noreply@gmail.com",
)
```

------------------------------------------------------------------------

### FastKafka

>      FastKafka (title:Optional[str]=None, description:Optional[str]=None,
>                 version:Optional[str]=None,
>                 contact:Optional[Dict[str,str]]=None,
>                 kafka_brokers:Dict[str,Any],
>                 root_path:Union[pathlib.Path,str,NoneType]=None, lifespan:Opti
>                 onal[Callable[[ForwardRef('FastKafka')],AsyncContextManager[No
>                 neType]]]=None, loop=None, client_id=None,
>                 metadata_max_age_ms=300000, request_timeout_ms=40000,
>                 api_version='auto', acks=<object object at 0x7f65242142f0>,
>                 key_serializer=None, value_serializer=None,
>                 compression_type=None, max_batch_size=16384,
>                 partitioner=<kafka.partitioner.default.DefaultPartitioner
>                 object at 0x7f65240e4a90>, max_request_size=1048576,
>                 linger_ms=0, send_backoff_ms=100, retry_backoff_ms=100,
>                 security_protocol='PLAINTEXT', ssl_context=None,
>                 connections_max_idle_ms=540000, enable_idempotence=False,
>                 transactional_id=None, transaction_timeout_ms=60000,
>                 sasl_mechanism='PLAIN', sasl_plain_password=None,
>                 sasl_plain_username=None, sasl_kerberos_service_name='kafka',
>                 sasl_kerberos_domain_name=None,
>                 sasl_oauth_token_provider=None, group_id=None,
>                 key_deserializer=None, value_deserializer=None,
>                 fetch_max_wait_ms=500, fetch_max_bytes=52428800,
>                 fetch_min_bytes=1, max_partition_fetch_bytes=1048576,
>                 auto_offset_reset='latest', enable_auto_commit=True,
>                 auto_commit_interval_ms=5000, check_crcs=True,
>                 partition_assignment_strategy=(<class 'kafka.coordinator.assig
>                 nors.roundrobin.RoundRobinPartitionAssignor'>,),
>                 max_poll_interval_ms=300000, rebalance_timeout_ms=None,
>                 session_timeout_ms=10000, heartbeat_interval_ms=3000,
>                 consumer_timeout_ms=200, max_poll_records=None,
>                 exclude_internal_topics=True,
>                 isolation_level='read_uncommitted')

Creates FastKafka application

Args: title: optional title for the documentation. If None, the title
will be set to empty string description: optional description for the
documentation. If None, the description will be set to empty string
version: optional version for the documentation. If None, the version
will be set to empty string contact: optional contact for the
documentation. If None, the contact will be set to placeholder values:
name=‘Author’ url=HttpUrl(’ https://www.google.com ‘, )
email=’noreply@gmail.com’ kafka_brokers: dictionary describing kafka
brokers used for generating documentation root_path: path to where
documentation will be created lifespan: asynccontextmanager that is used
for setting lifespan hooks. **aenter** is called before app start and
**aexit** after app stop. The lifespan is called whe application is
started as async context manager, e.g.:`async with kafka_app...`
client_id (str): a name for this client. This string is passed in each
request to servers and can be used to identify specific server-side log
entries that correspond to this client. Default: `aiokafka-producer-#`
(appended with a unique number per instance) key_serializer (Callable):
used to convert user-supplied keys to bytes If not :data:`None`, called
as `f(key),` should return :class:`bytes`. Default: :data:`None`.
value_serializer (Callable): used to convert user-supplied message
values to :class:`bytes`. If not :data:`None`, called as `f(value)`,
should return :class:`bytes`. Default: :data:`None`. acks (Any): one of
`0`, `1`, `all`. The number of acknowledgments the producer requires the
leader to have received before considering a request complete. This
controls the durability of records that are sent. The following settings
are common:

        * ``0``: Producer will not wait for any acknowledgment from the server
          at all. The message will immediately be added to the socket
          buffer and considered sent. No guarantee can be made that the
          server has received the record in this case, and the retries
          configuration will not take effect (as the client won't
          generally know of any failures). The offset given back for each
          record will always be set to -1.
        * ``1``: The broker leader will write the record to its local log but
          will respond without awaiting full acknowledgement from all
          followers. In this case should the leader fail immediately
          after acknowledging the record but before the followers have
          replicated it then the record will be lost.
        * ``all``: The broker leader will wait for the full set of in-sync
          replicas to acknowledge the record. This guarantees that the
          record will not be lost as long as at least one in-sync replica
          remains alive. This is the strongest available guarantee.

        If unset, defaults to ``acks=1``. If `enable_idempotence` is
        :data:`True` defaults to ``acks=all``
    compression_type (str): The compression type for all data generated by
        the producer. Valid values are ``gzip``, ``snappy``, ``lz4``, ``zstd``
        or :data:`None`.
        Compression is of full batches of data, so the efficacy of batching
        will also impact the compression ratio (more batching means better
        compression). Default: :data:`None`.
    max_batch_size (int): Maximum size of buffered data per partition.
        After this amount :meth:`send` coroutine will block until batch is
        drained.
        Default: 16384
    linger_ms (int): The producer groups together any records that arrive
        in between request transmissions into a single batched request.
        Normally this occurs only under load when records arrive faster
        than they can be sent out. However in some circumstances the client
        may want to reduce the number of requests even under moderate load.
        This setting accomplishes this by adding a small amount of
        artificial delay; that is, if first request is processed faster,
        than `linger_ms`, producer will wait ``linger_ms - process_time``.
        Default: 0 (i.e. no delay).
    partitioner (Callable): Callable used to determine which partition
        each message is assigned to. Called (after key serialization):
        ``partitioner(key_bytes, all_partitions, available_partitions)``.
        The default partitioner implementation hashes each non-None key
        using the same murmur2 algorithm as the Java client so that
        messages with the same key are assigned to the same partition.
        When a key is :data:`None`, the message is delivered to a random partition
        (filtered to partitions with available leaders only, if possible).
    max_request_size (int): The maximum size of a request. This is also
        effectively a cap on the maximum record size. Note that the server
        has its own cap on record size which may be different from this.
        This setting will limit the number of record batches the producer
        will send in a single request to avoid sending huge requests.
        Default: 1048576.
    metadata_max_age_ms (int): The period of time in milliseconds after
        which we force a refresh of metadata even if we haven't seen any
        partition leadership changes to proactively discover any new
        brokers or partitions. Default: 300000
    request_timeout_ms (int): Produce request timeout in milliseconds.
        As it's sent as part of
        :class:`~kafka.protocol.produce.ProduceRequest` (it's a blocking
        call), maximum waiting time can be up to ``2 *
        request_timeout_ms``.
        Default: 40000.
    retry_backoff_ms (int): Milliseconds to backoff when retrying on
        errors. Default: 100.
    api_version (str): specify which kafka API version to use.
        If set to ``auto``, will attempt to infer the broker version by
        probing various APIs. Default: ``auto``
    security_protocol (str): Protocol used to communicate with brokers.
        Valid values are: ``PLAINTEXT``, ``SSL``. Default: ``PLAINTEXT``.
        Default: ``PLAINTEXT``.
    ssl_context (ssl.SSLContext): pre-configured :class:`~ssl.SSLContext`
        for wrapping socket connections. Directly passed into asyncio's
        :meth:`~asyncio.loop.create_connection`. For more
        information see :ref:`ssl_auth`.
        Default: :data:`None`
    connections_max_idle_ms (int): Close idle connections after the number
        of milliseconds specified by this config. Specifying :data:`None` will
        disable idle checks. Default: 540000 (9 minutes).
    enable_idempotence (bool): When set to :data:`True`, the producer will
        ensure that exactly one copy of each message is written in the
        stream. If :data:`False`, producer retries due to broker failures,
        etc., may write duplicates of the retried message in the stream.
        Note that enabling idempotence acks to set to ``all``. If it is not
        explicitly set by the user it will be chosen. If incompatible
        values are set, a :exc:`ValueError` will be thrown.
        New in version 0.5.0.
    sasl_mechanism (str): Authentication mechanism when security_protocol
        is configured for ``SASL_PLAINTEXT`` or ``SASL_SSL``. Valid values
        are: ``PLAIN``, ``GSSAPI``, ``SCRAM-SHA-256``, ``SCRAM-SHA-512``,
        ``OAUTHBEARER``.
        Default: ``PLAIN``
    sasl_plain_username (str): username for SASL ``PLAIN`` authentication.
        Default: :data:`None`
    sasl_plain_password (str): password for SASL ``PLAIN`` authentication.
        Default: :data:`None`
    sasl_oauth_token_provider (: class:`~aiokafka.abc.AbstractTokenProvider`):
        OAuthBearer token provider instance. (See
        :mod:`kafka.oauth.abstract`).
        Default: :data:`None`
    *topics (list(str)): optional list of topics to subscribe to. If not set,
        call :meth:`.subscribe` or :meth:`.assign` before consuming records.
        Passing topics directly is same as calling :meth:`.subscribe` API.
    group_id (str or None): name of the consumer group to join for dynamic
        partition assignment (if enabled), and to use for fetching and
        committing offsets. If None, auto-partition assignment (via
        group coordinator) and offset commits are disabled.
        Default: None
    key_deserializer (Callable): Any callable that takes a
        raw message key and returns a deserialized key.
    value_deserializer (Callable, Optional): Any callable that takes a
        raw message value and returns a deserialized value.
    fetch_min_bytes (int): Minimum amount of data the server should
        return for a fetch request, otherwise wait up to
        `fetch_max_wait_ms` for more data to accumulate. Default: 1.
    fetch_max_bytes (int): The maximum amount of data the server should
        return for a fetch request. This is not an absolute maximum, if
        the first message in the first non-empty partition of the fetch
        is larger than this value, the message will still be returned
        to ensure that the consumer can make progress. NOTE: consumer
        performs fetches to multiple brokers in parallel so memory
        usage will depend on the number of brokers containing
        partitions for the topic.
        Supported Kafka version >= 0.10.1.0. Default: 52428800 (50 Mb).
    fetch_max_wait_ms (int): The maximum amount of time in milliseconds
        the server will block before answering the fetch request if
        there isn't sufficient data to immediately satisfy the
        requirement given by fetch_min_bytes. Default: 500.
    max_partition_fetch_bytes (int): The maximum amount of data
        per-partition the server will return. The maximum total memory
        used for a request ``= #partitions * max_partition_fetch_bytes``.
        This size must be at least as large as the maximum message size
        the server allows or else it is possible for the producer to
        send messages larger than the consumer can fetch. If that
        happens, the consumer can get stuck trying to fetch a large
        message on a certain partition. Default: 1048576.
    max_poll_records (int): The maximum number of records returned in a
        single call to :meth:`.getmany`. Defaults ``None``, no limit.
    auto_offset_reset (str): A policy for resetting offsets on
        :exc:`.OffsetOutOfRangeError` errors: ``earliest`` will move to the oldest
        available message, ``latest`` will move to the most recent, and
        ``none`` will raise an exception so you can handle this case.
        Default: ``latest``.
    enable_auto_commit (bool): If true the consumer's offset will be
        periodically committed in the background. Default: True.
    auto_commit_interval_ms (int): milliseconds between automatic
        offset commits, if enable_auto_commit is True. Default: 5000.
    check_crcs (bool): Automatically check the CRC32 of the records
        consumed. This ensures no on-the-wire or on-disk corruption to
        the messages occurred. This check adds some overhead, so it may
        be disabled in cases seeking extreme performance. Default: True
    partition_assignment_strategy (list): List of objects to use to
        distribute partition ownership amongst consumer instances when
        group management is used. This preference is implicit in the order
        of the strategies in the list. When assignment strategy changes:
        to support a change to the assignment strategy, new versions must
        enable support both for the old assignment strategy and the new
        one. The coordinator will choose the old assignment strategy until
        all members have been updated. Then it will choose the new
        strategy. Default: [:class:`.RoundRobinPartitionAssignor`]
    max_poll_interval_ms (int): Maximum allowed time between calls to
        consume messages (e.g., :meth:`.getmany`). If this interval
        is exceeded the consumer is considered failed and the group will
        rebalance in order to reassign the partitions to another consumer
        group member. If API methods block waiting for messages, that time
        does not count against this timeout. See `KIP-62`_ for more
        information. Default 300000
    rebalance_timeout_ms (int): The maximum time server will wait for this
        consumer to rejoin the group in a case of rebalance. In Java client
        this behaviour is bound to `max.poll.interval.ms` configuration,
        but as ``aiokafka`` will rejoin the group in the background, we
        decouple this setting to allow finer tuning by users that use
        :class:`.ConsumerRebalanceListener` to delay rebalacing. Defaults
        to ``session_timeout_ms``
    session_timeout_ms (int): Client group session and failure detection
        timeout. The consumer sends periodic heartbeats
        (`heartbeat.interval.ms`) to indicate its liveness to the broker.
        If no hearts are received by the broker for a group member within
        the session timeout, the broker will remove the consumer from the
        group and trigger a rebalance. The allowed range is configured with
        the **broker** configuration properties
        `group.min.session.timeout.ms` and `group.max.session.timeout.ms`.
        Default: 10000
    heartbeat_interval_ms (int): The expected time in milliseconds
        between heartbeats to the consumer coordinator when using
        Kafka's group management feature. Heartbeats are used to ensure
        that the consumer's session stays active and to facilitate
        rebalancing when new consumers join or leave the group. The
        value must be set lower than `session_timeout_ms`, but typically
        should be set no higher than 1/3 of that value. It can be
        adjusted even lower to control the expected time for normal
        rebalances. Default: 3000
    consumer_timeout_ms (int): maximum wait timeout for background fetching
        routine. Mostly defines how fast the system will see rebalance and
        request new data for new partitions. Default: 200
    exclude_internal_topics (bool): Whether records from internal topics
        (such as offsets) should be exposed to the consumer. If set to True
        the only way to receive records from an internal topic is
        subscribing to it. Requires 0.10+ Default: True
    isolation_level (str): Controls how to read messages written
        transactionally.

        If set to ``read_committed``, :meth:`.getmany` will only return
        transactional messages which have been committed.
        If set to ``read_uncommitted`` (the default), :meth:`.getmany` will
        return all messages, even transactional messages which have been
        aborted.

        Non-transactional messages will be returned unconditionally in
        either mode.

        Messages will always be returned in offset order. Hence, in
        `read_committed` mode, :meth:`.getmany` will only return
        messages up to the last stable offset (LSO), which is the one less
        than the offset of the first open transaction. In particular any
        messages appearing after messages belonging to ongoing transactions
        will be withheld until the relevant transaction has been completed.
        As a result, `read_committed` consumers will not be able to read up
        to the high watermark when there are in flight transactions.
        Further, when in `read_committed` the seek_to_end method will
        return the LSO. See method docs below. Default: ``read_uncommitted``
    sasl_oauth_token_provider (~aiokafka.abc.AbstractTokenProvider): OAuthBearer token provider instance. (See :mod:`kafka.oauth.abstract`).
        Default: None

``` python
assert FastKafka.__module__ == "fastkafka"
```

``` python
kafka_app = FastKafka(kafka_brokers=dict(localhost=dict(url="localhost", port=9092)))
kafka_app.__dict__
```

    {'_title': '',
     '_description': '',
     '_version': '',
     '_contact_info': ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),
     '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),
     '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='localhost', description='Kafka broker', port='9092', protocol='kafka', security=None)}),
     '_root_path': PosixPath('.'),
     '_asyncapi_path': PosixPath('asyncapi'),
     '_kafka_config': {'bootstrap_servers': 'localhost:9092',
      'auto_offset_reset': 'earliest',
      'max_poll_records': 100},
     '_consumers_store': {},
     '_producers_store': {},
     '_producers_list': [],
     'benchmark_results': {},
     '_scheduled_bg_tasks': [],
     '_bg_task_group_generator': None,
     '_bg_tasks_group': None,
     '_on_error_topic': None,
     'lifespan': None,
     'lifespan_ctx': None,
     '_is_started': False,
     '_is_shutting_down': False,
     '_kafka_consumer_tasks': [],
     '_kafka_producer_tasks': [],
     '_running_bg_tasks': [],
     'run': False,
     'AppMocks': None,
     'mocks': None,
     'awaited_mocks': None}

``` python
kafka_app = FastKafka(
    contact={"name": "Davor"},
    kafka_brokers=dict(localhost=dict(url="localhost", port=9092)),
)
kafka_app.__dict__
```

    {'_title': '',
     '_description': '',
     '_version': '',
     '_contact_info': ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com'),
     '_kafka_service_info': KafkaServiceInfo(title='', version='', description='', contact=ContactInfo(name='Davor', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')),
     '_kafka_brokers': KafkaBrokers(brokers={'localhost': KafkaBroker(url='localhost', description='Kafka broker', port='9092', protocol='kafka', security=None)}),
     '_root_path': PosixPath('.'),
     '_asyncapi_path': PosixPath('asyncapi'),
     '_kafka_config': {'bootstrap_servers': 'localhost:9092',
      'auto_offset_reset': 'earliest',
      'max_poll_records': 100},
     '_consumers_store': {},
     '_producers_store': {},
     '_producers_list': [],
     'benchmark_results': {},
     '_scheduled_bg_tasks': [],
     '_bg_task_group_generator': None,
     '_bg_tasks_group': None,
     '_on_error_topic': None,
     'lifespan': None,
     'lifespan_ctx': None,
     '_is_started': False,
     '_is_shutting_down': False,
     '_kafka_consumer_tasks': [],
     '_kafka_producer_tasks': [],
     '_running_bg_tasks': [],
     'run': False,
     'AppMocks': None,
     'mocks': None,
     'awaited_mocks': None}

``` python
def create_testing_app(
    *, root_path: str = "/tmp/000_FastKafka", bootstrap_servers: Optional[str] = None
):
    if Path(root_path).exists():
        shutil.rmtree(root_path)

    host, port = None, None
    if bootstrap_servers is not None:
        host, port = bootstrap_servers.split(":")

    kafka_app = FastKafka(
        kafka_brokers={
            "localhost": {
                "url": host if host is not None else "localhost",
                "name": "development",
                "description": "Local (dev) Kafka broker",
                "port": port if port is not None else 9092,
            }
        },
        root_path=root_path,
    )
    kafka_app.set_kafka_broker(kafka_broker_name="localhost")

    return kafka_app
```

``` python
app = create_testing_app()
assert Path("/tmp/000_FastKafka").exists()
app
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'

    <fastkafka.FastKafka>

``` python
actual = _get_decoder_fn("json")
assert actual == json_decoder

actual = _get_decoder_fn("avro")
assert actual == avro_decoder
```

------------------------------------------------------------------------

### consumes

>      consumes (topic:Optional[str]=None, decoder:Union[str,Callable[[bytes,pyd
>                antic.main.ModelMetaclass],Any]]='json', executor:Union[str,fas
>                tkafka._components.task_streaming.StreamExecutor,NoneType]=None
>                , prefix:str='on_', **kwargs:Dict[str,Any])

Decorator registering the callback called when a message is received in
a topic.

This function decorator is also responsible for registering topics for
AsyncAPI specificiation and documentation.

Args: topic: Kafka topic that the consumer will subscribe to and execute
the decorated function when it receives a message from the topic,
default: None. If the topic is not specified, topic name will be
inferred from the decorated function name by stripping the defined
prefix decoder: Decoder to use to decode messages consumed from the
topic, default: json - By default, it uses json decoder to decode bytes
to json string and then it creates instance of pydantic BaseModel. It
also accepts custom decoder function. executor: Type of executor to
choose for consuming tasks. Avaliable options are “SequentialExecutor"
and “DynamicTaskExecutor". The default option is “SequentialExecutor"
which will execute the consuming tasks sequentially. If the consuming
tasks have high latency it is recommended to use “DynamicTaskExecutor"
which will wrap the consuming functions into tasks and run them in on
asyncio loop in background. This comes with a cost of increased overhead
so use it only in cases when your consume functions have high latency
such as database queries or some other type of networking. prefix:
Prefix stripped from the decorated function to define a topic name if
the topic argument is not passed, default: “on\_". If the decorated
function name is not prefixed with the defined prefix and topic argument
is not passed, then this method will throw ValueError

Returns: A function returning the same function

Throws: ValueError

``` python
app = create_testing_app()


# Basic check
@app.consumes()
def on_my_topic_1(msg: BaseModel) -> None:
    pass


assert app._consumers_store["my_topic_1"] == (
    on_my_topic_1,
    json_decoder,
    None,
    {},
), app._consumers_store

assert hasattr(app, "on_my_topic_1")

# Check executor setting
@app.consumes(executor="DynamicTaskExecutor")
def on_my_topic_12(msg: BaseModel) -> None:
    pass

assert app._consumers_store["my_topic_12"] == (
    on_my_topic_12,
    json_decoder,
    "DynamicTaskExecutor",
    {},
), app._consumers_store["my_topic_12"]

assert hasattr(app, "on_my_topic_12")

# Check topic setting
@app.consumes(topic="test_topic_2")
def some_func_name(msg: BaseModel) -> None:
    pass

assert app._consumers_store["test_topic_2"] == (
    some_func_name,
    json_decoder,
    None,
    {},
), app._consumers_store

# Check prefix change
@app.consumes(prefix="for_")
def for_test_topic_3(msg: BaseModel) -> None:
    pass


assert app._consumers_store["test_topic_3"] == (
    for_test_topic_3,
    json_decoder,
    None,
    {},
), app._consumers_store

assert hasattr(app, "for_test_topic_3")

# Check passing of kwargs
kwargs = {"arg1": "val1", "arg2": 2}


@app.consumes(topic="test_topic", **kwargs)
def for_test_kwargs(msg: BaseModel):
    pass


assert app._consumers_store["test_topic"] == (
    for_test_kwargs,
    json_decoder,
    None,
    kwargs,
), app._consumers_store

assert hasattr(app, "for_test_kwargs")
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'

``` python
actual = _get_encoder_fn("json")
assert actual == json_encoder

actual = _get_encoder_fn("avro")
assert actual == avro_encoder
```

------------------------------------------------------------------------

### produces

>      produces (topic:Optional[str]=None,
>                encoder:Union[str,Callable[[pydantic.main.BaseModel],bytes]]='j
>                son', prefix:str='to_', **kwargs:Dict[str,Any])

Decorator registering the callback called when delivery report for a
produced message is received

This function decorator is also responsible for registering topics for
AsyncAPI specificiation and documentation.

Args: topic: Kafka topic that the producer will send returned values
from the decorated function to, default: None- If the topic is not
specified, topic name will be inferred from the decorated function name
by stripping the defined prefix. encoder: Encoder to use to encode
messages before sending it to topic, default: json - By default, it uses
json encoder to convert pydantic basemodel to json string and then
encodes the string to bytes using ‘utf-8’ encoding. It also accepts
custom encoder function. prefix: Prefix stripped from the decorated
function to define a topic name if the topic argument is not passed,
default: “to\_". If the decorated function name is not prefixed with the
defined prefix and topic argument is not passed, then this method will
throw ValueError

Returns: A function returning the same function

Raises: ValueError: when needed

``` python
app = create_testing_app()


# Basic check
async def to_my_topic_1(msg: BaseModel) -> None:
    pass


# Must be done without sugar to keep the original function reference
check_func = to_my_topic_1
to_my_topic_1 = app.produces()(to_my_topic_1)

assert app._producers_store["my_topic_1"] == (
    check_func,
    None,
    {},
), f"{app._producers_store}, {to_my_topic_1}"

assert hasattr(app, "to_my_topic_1")

# Check topic setting
def some_func_name(msg: BaseModel) -> None:
    pass


check_func = some_func_name
some_func_name = app.produces(topic="test_topic_2")(some_func_name)

assert app._producers_store["test_topic_2"] == (
    check_func,
    None,
    {},
), app._producers_store

assert hasattr(app, "some_func_name")

# Check prefix change
@app.produces(prefix="for_")
def for_test_topic_3(msg: BaseModel) -> None:
    pass


check_func = for_test_topic_3
some_func_name = app.produces(prefix="for_")(for_test_topic_3)

assert app._producers_store["test_topic_3"] == (
    check_func,
    None,
    {},
), app._producers_store

# Check passing of kwargs
kwargs = {"arg1": "val1", "arg2": 2}

assert hasattr(app, "for_test_topic_3")

async def for_test_kwargs(msg: BaseModel):
    pass


check_func = for_test_kwargs
for_test_kwargs = app.produces(topic="test_topic", **kwargs)(for_test_kwargs)

assert app._producers_store["test_topic"] == (
    check_func,
    None,
    kwargs,
), app._producers_store

assert hasattr(app, "for_test_kwargs")
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L522"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### FastKafka.get_topics

>      FastKafka.get_topics ()

``` python
app = create_testing_app()


@app.produces()
def to_topic_1() -> BaseModel:
    pass


@app.consumes()
def on_topic_2(msg: BaseModel):
    pass


assert app.get_topics() == set(["topic_1", "topic_2"]), f"{app.get_topics()=}"
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L529"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### FastKafka.run_in_background

>      FastKafka.run_in_background ()

Decorator to schedule a task to be run in the background.

This decorator is used to schedule a task to be run in the background
when the app’s `_on_startup` event is triggered.

Returns: Callable\[None, None\]: A decorator function that takes a
background task as an input and stores it to be run in the backround.

``` python
# Check if the background job is getting registered

app = create_testing_app()


@app.run_in_background()
async def async_background_job():
    """Async background job"""
    pass


assert app._scheduled_bg_tasks[0] == async_background_job, app._scheduled_bg_tasks[0]
assert app._scheduled_bg_tasks.__len__() == 1
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'
    [INFO] __main__: run_in_background() : Adding function 'async_background_job' as background task

``` python
class MyInfo(BaseModel):
    mobile: str = Field(..., example="+385987654321")
    name: str = Field(..., example="James Bond")


class MyMsgUrl(BaseModel):
    info: MyInfo = Field(..., example=dict(mobile="+385987654321", name="James Bond"))
    url: HttpUrl = Field(..., example="https://sis.gov.uk/agents/007")


class MyMsgEmail(BaseModel):
    msg_url: MyMsgUrl = Field(
        ...,
        example=dict(
            info=dict(mobile="+385987654321", name="James Bond"),
            url="https://sis.gov.uk/agents/007",
        ),
    )
    email: EmailStr = Field(..., example="agent-007@sis.gov.uk")


def setup_testing_app(bootstrap_servers=None):
    app = create_testing_app(bootstrap_servers=bootstrap_servers)

    @app.consumes("my_topic_1")
    def on_my_topic_one(msg: MyMsgUrl) -> None:
        logger.debug(f"on_my_topic_one(msg={msg},)")

    @app.consumes()
    async def on_my_topic_2(msg: MyMsgEmail) -> None:
        logger.debug(f"on_my_topic_2(msg={msg},)")

    with pytest.raises(ValueError) as e:

        @app.consumes()
        def my_topic_3(msg: MyMsgEmail) -> None:
            raise NotImplemented

    @app.produces()
    def to_my_topic_3(url: str) -> MyMsgUrl:
        logger.debug(f"on_my_topic_3(msg={url}")
        return MyMsgUrl(info=MyInfo("+3851987654321", "Sean Connery"), url=url)

    @app.produces()
    async def to_my_topic_4(msg: MyMsgEmail) -> MyMsgEmail:
        logger.debug(f"on_my_topic_4(msg={msg}")
        return msg

    @app.produces()
    def to_my_topic_5(url: str) -> MyMsgUrl:
        logger.debug(f"on_my_topic_5(msg={url}")
        return MyMsgUrl(info=MyInfo("+3859123456789", "John Wayne"), url=url)

    @app.run_in_background()
    async def long_bg_job():
        logger.debug(f"long_bg_job()")
        await asyncio.sleep(100)

    return app
```

``` python
app = setup_testing_app()

assert set(app._consumers_store.keys()) == set(["my_topic_1", "my_topic_2"])
assert set(app._producers_store.keys()) == set(
    ["my_topic_3", "my_topic_4", "my_topic_5"]
)

print(f"app._kafka_service_info={app._kafka_service_info}")
print(f"app._kafka_brokers={app._kafka_brokers}")
```

    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to 'localhost:9092'
    [INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task
    app._kafka_service_info=title='' version='' description='' contact=ContactInfo(name='Author', url=HttpUrl('https://www.google.com', ), email='noreply@gmail.com')
    app._kafka_brokers=brokers={'localhost': KafkaBroker(url='localhost', description='Local (dev) Kafka broker', port='9092', protocol='kafka', security=None)}

``` python
async with ApacheKafkaBroker() as bootstrap_server:
    app = setup_testing_app(bootstrap_servers=bootstrap_server)
    app._populate_consumers(is_shutting_down_f=true_after(1))
    assert len(app._kafka_consumer_tasks) == 2

    await app._shutdown_consumers()

    assert all([t.done() for t in app._kafka_consumer_tasks])
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._components.test_dependencies: But not exported to PATH, exporting...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_2'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_2'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'my_topic_1'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'my_topic_1'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [WARNING] aiokafka.cluster: Topic my_topic_2 is not available during auto-create initialization
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_2': 0}. 
    [WARNING] aiokafka.cluster: Topic my_topic_1 is not available during auto-create initialization
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'my_topic_1': 0}. 
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 39594...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 39594 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 39234...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 39234 terminated.

``` python
async with ApacheKafkaBroker() as bootstrap_server:
    app = setup_testing_app(bootstrap_servers=bootstrap_server)
    print(app._producers_store)
    await app._populate_producers()
    print(app._producers_store)
    assert len(app._producers_list) == 3
    print(app._producers_list)
    await app._shutdown_producers()

    # One more time for reentrancy
    await app._populate_producers()
    assert len(app._producers_list) == 3
    print(app._producers_list)
    await app._shutdown_producers()
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task
    {'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, None, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, None, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, None, {})}
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    {'my_topic_3': (<function setup_testing_app.<locals>.to_my_topic_3>, <aiokafka.producer.producer.AIOKafkaProducer object>, {}), 'my_topic_4': (<function setup_testing_app.<locals>.to_my_topic_4>, <aiokafka.producer.producer.AIOKafkaProducer object>, {}), 'my_topic_5': (<function setup_testing_app.<locals>.to_my_topic_5>, <aiokafka.producer.producer.AIOKafkaProducer object>, {})}
    [<aiokafka.producer.producer.AIOKafkaProducer object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <aiokafka.producer.producer.AIOKafkaProducer object>]
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [<aiokafka.producer.producer.AIOKafkaProducer object>, <aiokafka.producer.producer.AIOKafkaProducer object>, <aiokafka.producer.producer.AIOKafkaProducer object>]
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 40393...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 40393 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 40032...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 40032 terminated.

``` python
async with ApacheKafkaBroker() as bootstrap_server:
    app = setup_testing_app(bootstrap_servers=bootstrap_server)

    @app.run_in_background()
    async def long_bg_job():
        logger.debug(f"new_long_bg_job()")
        await asyncio.sleep(100)

    await app._populate_bg_tasks()
    assert len(app._scheduled_bg_tasks) == 2
    assert len(app._running_bg_tasks) == 2
    await app._shutdown_bg_tasks()
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task
    [INFO] __main__: run_in_background() : Adding function 'long_bg_job' as background task
    [INFO] __main__: _populate_bg_tasks() : Starting background task 'long_bg_job'
    [INFO] __main__: _populate_bg_tasks() : Starting background task 'long_bg_job'
    [INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'long_bg_job'
    [INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'long_bg_job'
    [INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'long_bg_job' to finish
    [INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'long_bg_job'
    [INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'long_bg_job' to finish
    [INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'long_bg_job'
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 41188...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 41188 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 40828...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 40828 terminated.

``` python
# Test app reentrancy

async with ApacheKafkaBroker() as bootstrap_server:
    with mock_AIOKafkaProducer_send() as mock:
        app = create_testing_app(bootstrap_servers=bootstrap_server)

        @app.produces()
        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:
            msg = MyMsgUrl(info=dict(mobile=mobile, name="James Bond"), url=url)
            return msg

        try:
            await app._start()
            await app.to_my_test_topic(mobile="+385912345678", url="https://www.vip.hr")
        finally:
            await app._stop()

        try:
            await app._start()
            await app.to_my_test_topic(mobile="+385987654321", url="https://www.ht.hr")
        finally:
            await app._stop()

        mock.assert_has_calls(
            [
                unittest.mock.call(
                    "my_test_topic",
                    b'{"info": {"mobile": "+385912345678", "name": "James Bond"}, "url": "https://www.vip.hr"}',
                    key=None,
                ),
                unittest.mock.call(
                    "my_test_topic",
                    b'{"info": {"mobile": "+385987654321", "name": "James Bond"}, "url": "https://www.ht.hr"}',
                    key=None,
                ),
            ]
        )
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 41985...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 41985 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 41624...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 41624 terminated.

``` python
# mock up send method of AIOKafkaProducer
async with ApacheKafkaBroker() as bootstrap_server:
    with mock_AIOKafkaProducer_send() as mock:
        app = create_testing_app(bootstrap_servers=bootstrap_server)

        @app.produces()
        async def to_my_test_topic(mobile: str, url: str) -> MyMsgUrl:
            msg = MyMsgUrl(info=dict(mobile=mobile, name="James Bond"), url=url)
            return msg

        @app.produces()
        def to_my_test_topic_2(mobile: str, url: str) -> MyMsgUrl:
            msg = MyMsgUrl(info=dict(mobile=mobile, name="James Bond"), url=url)
            return msg

        try:
            await app._start()
            await to_my_test_topic(mobile="+385912345678", url="https://www.vip.hr")
            to_my_test_topic_2(mobile="+385987654321", url="https://www.ht.hr")
        finally:
            await app._stop()

        mock.assert_has_calls(
            [
                unittest.mock.call(
                    "my_test_topic",
                    b'{"info": {"mobile": "+385912345678", "name": "James Bond"}, "url": "https://www.vip.hr"}',
                    key=None,
                ),
                unittest.mock.call(
                    "my_test_topic_2",
                    b'{"info": {"mobile": "+385987654321", "name": "James Bond"}, "url": "https://www.ht.hr"}',
                    key=None,
                ),
            ]
        )
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [WARNING] aiokafka.cluster: No broker metadata found in MetadataResponse
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 42781...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 42781 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 42420...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 42420 terminated.

``` python
async with ApacheKafkaBroker() as bootstrap_server:
    app = create_testing_app(bootstrap_servers=bootstrap_server)
    fast_task = unittest.mock.Mock()
    long_task = unittest.mock.Mock()

    @app.run_in_background()
    async def bg_task():
        fast_task()
        await asyncio.sleep(100)
        long_task()

    fast_task_second = unittest.mock.Mock()
    long_task_second = unittest.mock.Mock()

    @app.run_in_background()
    async def bg_task_second():
        fast_task_second()
        await asyncio.sleep(100)
        long_task_second()

    try:
        await app._start()
        await asyncio.sleep(5)
    finally:
        await app._stop()

    fast_task.assert_called()
    long_task.assert_not_called()

    fast_task_second.assert_called()
    long_task_second.assert_not_called()

print("ok")
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] __main__: run_in_background() : Adding function 'bg_task' as background task
    [INFO] __main__: run_in_background() : Adding function 'bg_task_second' as background task
    [INFO] __main__: _populate_bg_tasks() : Starting background task 'bg_task'
    [INFO] __main__: _populate_bg_tasks() : Starting background task 'bg_task_second'
    [INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'bg_task'
    [INFO] __main__: _shutdown_bg_tasks() : Cancelling background task 'bg_task_second'
    [INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'bg_task' to finish
    [INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'bg_task'
    [INFO] __main__: _shutdown_bg_tasks() : Waiting for background task 'bg_task_second' to finish
    [INFO] __main__: _shutdown_bg_tasks() : Execution finished for background task 'bg_task_second'
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 43574...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 43574 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 43214...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 43214 terminated.
    ok

``` python
# test lifespan hook

global_dict = {}


@asynccontextmanager
async def lifespan(app: FastKafka):
    try:
        global_dict["set_var"] = 123
        global_dict["app"] = app
        yield
    finally:
        global_dict["set_var"] = 321


with ApacheKafkaBroker(apply_nest_asyncio=True) as bootstrap_servers:
    host, port = bootstrap_servers.split(":")

    kafka_app = FastKafka(
        kafka_brokers={
            "localhost": {
                "url": host if host is not None else "localhost",
                "name": "development",
                "description": "Local (dev) Kafka broker",
                "port": port if port is not None else 9092,
            }
        },
        root_path="/tmp/000_FastKafka",
        lifespan=lifespan,
    )

    kafka_app.set_kafka_broker(kafka_broker_name="localhost")

    # Dict unchanged
    assert global_dict == {}

    async with kafka_app:
        # Lifespan aenter triggered
        assert global_dict["set_var"] == 123
        # Kafka app reference passed
        assert global_dict["app"] == kafka_app

    # Lifespan aexit triggered
    assert global_dict["set_var"] == 321
```

    [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): entering...
    [WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): (<_UnixSelectorEventLoop running=True closed=False debug=False>) is already running!
    [WARNING] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): calling nest_asyncio.apply()
    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] fastkafka._testing.apache_kafka_broker: <class 'fastkafka.testing.ApacheKafkaBroker'>.start(): returning 127.0.0.1:9092
    [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.start(): exited.
    [INFO] __main__: set_kafka_broker() : Setting bootstrap_servers value to '127.0.0.1:9092'
    [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): entering...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 44371...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 44371 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 44010...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 44010 terminated.
    [INFO] fastkafka._testing.apache_kafka_broker: ApacheKafkaBroker.stop(): exited.

## Documentation generation

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L758"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### FastKafka.create_docs

>      FastKafka.create_docs ()

``` python
expected = """asyncapi: 2.5.0
channels:
  my_topic_1:
    subscribe:
      message:
        $ref: '#/components/messages/MyMsgUrl'
  my_topic_2:
    subscribe:
      message:
        $ref: '#/components/messages/MyMsgEmail'
  my_topic_3:
    publish:
      message:
        $ref: '#/components/messages/MyMsgUrl'
  my_topic_4:
    publish:
      message:
        $ref: '#/components/messages/MyMsgEmail'
  my_topic_5:
    publish:
      message:
        $ref: '#/components/messages/MyMsgUrl'
components:
  messages:
    MyMsgEmail:
      payload:
        example:
          email: agent-007@sis.gov.uk
          msg_url:
            info:
              mobile: '+385987654321'
              name: James Bond
            url: https://sis.gov.uk/agents/007
        properties:
          email:
            example: agent-007@sis.gov.uk
            format: email
            title: Email
            type: string
          msg_url:
            allOf:
            - $ref: '#/components/messages/MyMsgUrl'
            example:
              info:
                mobile: '+385987654321'
                name: James Bond
              url: https://sis.gov.uk/agents/007
            title: Msg Url
        required:
        - msg_url
        - email
        title: MyMsgEmail
        type: object
    MyMsgUrl:
      payload:
        example:
          info:
            mobile: '+385987654321'
            name: James Bond
          url: https://sis.gov.uk/agents/007
        properties:
          info:
            allOf:
            - $ref: '#/components/schemas/MyInfo'
            example:
              mobile: '+385987654321'
              name: James Bond
            title: Info
          url:
            example: https://sis.gov.uk/agents/007
            format: uri
            maxLength: 2083
            minLength: 1
            title: Url
            type: string
        required:
        - info
        - url
        title: MyMsgUrl
        type: object
  schemas:
    MyInfo:
      payload:
        properties:
          mobile:
            example: '+385987654321'
            title: Mobile
            type: string
          name:
            example: James Bond
            title: Name
            type: string
        required:
        - mobile
        - name
        title: MyInfo
        type: object
  securitySchemes: {}
info:
  contact:
    email: noreply@gmail.com
    name: Author
    url: https://www.google.com
  description: ''
  title: ''
  version: ''
servers:
  localhost:
    description: Local (dev) Kafka broker
    protocol: kafka
    url: localhost
    variables:
      port:
        default: '9092'
"""
```

``` python
d1, d2 = None, None

docs_path = Path("/tmp/000_FastKafka/asyncapi/spec/asyncapi.yml")
if docs_path.exists():
    os.remove(docs_path)


async def test_me():
    global d1
    global d2
    app = setup_testing_app()
    app.create_docs()
    with open(docs_path) as specs:
        d1 = yaml.safe_load(specs)
        d2 = yaml.safe_load(expected)
        assert d1 == d2, f"{d1} != {d2}"


asyncio.run(test_me())
print("ok")
```

## App mocks

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L773"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### AwaitedMock

>      AwaitedMock (o:Any)

Initialize self. See help(type(self)) for accurate signature.

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L810"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### FastKafka.create_mocks

>      FastKafka.create_mocks ()

Creates self.mocks as a named tuple mapping a new function obtained by
calling the original functions and a mock

``` python
class TestMsg(BaseModel):
    msg: str = Field(...)


app = FastKafka(kafka_brokers=dict(localhost=dict(url="localhost", port=9092)))


@app.consumes()
async def on_preprocessed_signals(msg: TestMsg):
    await to_predictions(TestMsg(msg="prediction"))


@app.produces()
async def to_predictions(prediction: TestMsg) -> TestMsg:
    print(f"Sending prediction: {prediction}")
    return prediction
```

``` python
app.create_mocks()
app.mocks.on_preprocessed_signals.assert_not_awaited()
app.mocks.to_predictions.assert_not_awaited()
app.create_mocks()
app.mocks.on_preprocessed_signals.assert_not_awaited()
app.mocks.to_predictions.assert_not_awaited()
```

``` python
with pytest.raises(AssertionError) as e:
    await app.awaited_mocks.on_preprocessed_signals.assert_called_with(123, timeout=2)
```

``` python
app.create_mocks()
app.mocks.on_preprocessed_signals.assert_not_awaited()
await app.awaited_mocks.on_preprocessed_signals.assert_not_awaited(timeout=3)
```

------------------------------------------------------------------------

<a
href="https://github.com/airtai/fastkafka/blob/main/fastkafka/_application/app.py#L879"
target="_blank" style={{float: 'right', fontSize: 'smaller'}}>source</a>

### FastKafka.benchmark

>      FastKafka.benchmark (interval:Union[int,datetime.timedelta]=1,
>                           sliding_window_size:Optional[int]=None)

Decorator to benchmark produces/consumes functions

Args: interval: Period to use to calculate throughput. If value is of
type int, then it will be used as seconds. If value is of type
timedelta, then it will be used as it is. default: 1 - one second
sliding_window_size: The size of the sliding window to use to calculate
average throughput. default: None - By default average throughput is not
calculated

``` python
for executor in ["SequentialExecutor", "DynamicTaskExecutor"]:
    class TestMsg(BaseModel):
        msg: str = Field(...)


    app = FastKafka(kafka_brokers=dict(localhost=dict(url="localhost", port=9092)))
    # app.benchmark_results["test"] = dict(count=0)


    @app.consumes(executor=executor)
    @app.benchmark(interval=1, sliding_window_size=5)
    async def on_preprocessed_signals(msg: TestMsg):
        await to_predictions(TestMsg(msg="prediction"))


    @app.produces()
    @app.benchmark(interval=1, sliding_window_size=5)
    async def to_predictions(prediction: TestMsg) -> TestMsg:
#         print(f"Sending prediction: {prediction}")
        return prediction


    async with Tester(app).using_local_kafka() as tester:
        for i in range(10_000):
            await tester.to_preprocessed_signals(TestMsg(msg=f"signal {i}"))
        print("Hello I am over after 10k msgs")
        await asyncio.sleep(5)
        tester.mocks.on_predictions.assert_called()

print("ok")
```

    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'preprocessed_signals'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'preprocessed_signals'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'preprocessed_signals': 1}. 
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. 
    Hello I am over after 10k msgs
    [INFO] fastkafka.benchmark: Throughput = 2,169, Avg throughput = 2,169 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 2,167, Avg throughput = 2,167 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,619, Avg throughput = 2,894 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,619, Avg throughput = 2,893 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,281, Avg throughput = 3,023 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,282, Avg throughput = 3,023 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 45164...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 45164 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 44804...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 44804 terminated.
    [INFO] fastkafka._components.test_dependencies: Java is already installed.
    [INFO] fastkafka._components.test_dependencies: Kafka is installed.
    [INFO] fastkafka._testing.apache_kafka_broker: Starting zookeeper...
    [INFO] fastkafka._testing.apache_kafka_broker: Starting kafka...
    [INFO] fastkafka._testing.apache_kafka_broker: Local Kafka broker up and running on 127.0.0.1:9092
    [INFO] __main__: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._application.app: _create_producer() : created producer using the config: '{'bootstrap_servers': '127.0.0.1:9092'}'
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() starting...
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer created using the following parameters: {'bootstrap_servers': '127.0.0.1:9092', 'auto_offset_reset': 'earliest', 'max_poll_records': 100}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'preprocessed_signals'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'preprocessed_signals'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'preprocessed_signals': 1}. 
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer started.
    [INFO] aiokafka.consumer.subscription_state: Updating subscribed topics to: frozenset({'predictions'})
    [INFO] aiokafka.consumer.consumer: Subscribed to topic(s): {'predictions'}
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer subscribed.
    [INFO] aiokafka.consumer.group_coordinator: Metadata for topic has changed from {} to {'predictions': 1}. 
    Hello I am over after 10k msgs
    [INFO] fastkafka.benchmark: Throughput = 1,910, Avg throughput = 1,910 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 1,908, Avg throughput = 1,908 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,197, Avg throughput = 2,554 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,198, Avg throughput = 2,553 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,355, Avg throughput = 2,821 - For __main__.on_preprocessed_signals(interval=1,sliding_window_size=5)
    [INFO] fastkafka.benchmark: Throughput = 3,356, Avg throughput = 2,820 - For __main__.to_predictions(interval=1,sliding_window_size=5)
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop(): Consumer stopped.
    [INFO] fastkafka._components.aiokafka_consumer_loop: aiokafka_consumer_loop() finished.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 46647...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 46647 terminated.
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Terminating the process 46285...
    [INFO] fastkafka._components._subprocess: terminate_asyncio_process(): Process 46285 terminated.
    ok
